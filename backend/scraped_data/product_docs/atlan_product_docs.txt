URL: https://docs.atlan.com/
Title: Atlan | Atlan Documentation
Content: Discover, trust, and govern your data & AI ecosystemEverything you need to get started with Atlan.Set up SnowflakeSet up DatabricksSet up Power BIAtlan ArchitectureBrowser Extension Get startedüöÄQuick-start guideStep-by-step onboardingüîßSecure agentEnterprise-grade deployment optionsüìùPlaybooks automationRule-based metadata updates at scaleCore featuresüîçFind & understand dataSearch, discover, and profile assetsüõ°Ô∏èGovern & manageCreate data contracts & policiesüîåIntegrateAutomation, collaboration & other integrationsDeveloper hub‚öôÔ∏èIntroductory walkthroughPlay with APIs in minutesüíªClient SDKsJava, Python & moreüì¶PackagesDeveloper-built utilities and integrations Atlan UniversityGet started with Atlan by building the right strategy and setting a strong foundation.Atlan SecurityA comprehensive look at Atlan's security philosophy, core values, and rigorous security proceduresHelp and supportFind answers or contact our team for personalized assistance
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake
Title: Set up Snowflake | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeGet StartedSet up SnowflakeOn this pageSet up SnowflakeWho can do this?You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake‚Äã Create a role and user in Snowflake using the following commands: Create role‚Äã Create a role in Snowflake using the following commands: CREATE OR REPLACE ROLE atlan_user_role;GRANT OPERATE, USAGE ON WAREHOUSE "<warehouse-name>" TO ROLE atlan_user_role; Replace <warehouse-name> with the default warehouse to use when running the Snowflake crawler. Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user‚Äã Create a separate user to integrate into Atlan, using one of the following 3 options: With a public key in Snowflake‚Äã See Snowflake's official guide for details on generating an RSA key-pair. To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: CREATE USER atlan_user rsa_public_key='MIIBIjANBgkqh...' default_role=atlan_user_role default_warehouse='<warehouse-name>' display_name='Atlan'TYPE = 'SERVICE' Learn more about the SERVICE type property in Snowflake documentation. Did you know?Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. With a password in Snowflake‚Äã Did you know?Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace <password> and run the following: CREATE USER atlan_user password='<password>' default_role=atlan_user_role default_warehouse='<warehouse-name>' display_name='Atlan'TYPE = 'LEGACY_SERVICE' Learn more about the LEGACY_SERVICE type property in Snowflake documentation. Managed through your identity provider (IdP) Private preview‚Äã This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta: Create a user in your identity provider (IdP) and use federated authentication in Snowflake. The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user‚Äã To grant the atlan_user_role to the new user: GRANT ROLE atlan_user_role TO USER atlan_user; Configure OAuth (client credentials flow) with Microsoft Entra ID‚Äã To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID, tenant ID, and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\<AZURE_AD_ISSUER\>' EXTERNAL_OAUTH_JWS_KEYS_URL = '\<AZURE_AD_JWS_KEY_ENDPOINT\>' EXTERNAL_OAUTH_AUDIENCE_LIST = ('\<SNOWFLAKE_APPLICATION_ID_URI\>') EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name'; Replace the placeholders with actual values from your Azure AD app: <AZURE_AD_ISSUER> ‚Üí Your tenant's OAuth 2.0 issuer URL <AZURE_AD_JWS_KEY_ENDPOINT> ‚Üí Azure JWKs URI <SNOWFLAKE_APPLICATION_ID_URI> ‚Üí Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\<AZURE_AD_CLIENT_OBJECT_ID\>' -- Use Azure client OBJECT ID DEFAULT_ROLE = \<ROLE\> DEFAULT_WAREHOUSE = \<WAREHOUSE\>; Grant the configured role to this user: GRANT ROLE \<ROLE\> TO USER oauth_svc_user; Choose metadata fetching method‚Äã Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Account usageInformation schemaOverviewSimplified grants but some limitations in functionalityMost comprehensive approach, more grant management requiredMethodViews in the SNOWFLAKE database that display object metadata and usage metrics for your accountSystem-defined views and table functions that provide extensive metadata for objects created in your accountPermissionsUser role and account, single grant for SNOWFLAKE databaseUser role and account, multiple grants per databaseData latency45 minutes to 3 hours (varies by view)NoneHistorical data retention1 year7 days to 6 months (varies by view or table function)Asset extractionACCOUNT_USAGE schemaINFORMATION_SCHEMA schemaView lineageACCOUNT_USAGE schemaINFORMATION_SCHEMA schemaTable lineageACCOUNT_USAGE schemaACCOUNT_USAGE schemaTag importACCOUNT_USAGE schemaACCOUNT_USAGE schemaUsage and popularityACCOUNT_USAGE schemaACCOUNT_USAGE schemaMetadata extraction timeVaries by warehouse size. For example, 8 minutes for 10 million assets (recommended for extracting a large number of assets)Varies by warehouse size. For example, 2+ hours for 10 million assetsExtraction limitationsExternal table location data, procedures, and primary and foreign keysNone Grant permissions for account usage method‚Äã warningIf you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach. To crawl assets, generate lineage, and import tags‚Äã If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner, you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher. To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: USE ROLE ACCOUNTADMIN;GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role; The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: GRANT USAGE ON DATABASE "<copied-database>" TO ROLE atlan_user_role;GRANT USAGE ON SCHEMA "<copied-schema>" IN DATABASE "<copied-database>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL VIEWS IN DATABASE "<copied-database>" TO ROLE atlan_user_role; Replace <copied-database> with the copied Snowflake database name. Replace <copied-schema> with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams‚Äã To crawl streams, provide the following permissions: To crawl current streams: GRANT USAGE ON ALL SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role; Replace <database-name> with the Snowflake database name. To crawl future streams: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role; Replace <database-name> with the Snowflake database name. (Optional) To preview and query existing assets‚Äã To query and preview data within assets that already exist in Snowflake, add these permissions: GRANT USAGE ON DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT USAGE ON ALL SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT MONITOR ON PIPE "<pipe-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets‚Äã To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT MONITOR ON FUTURE PIPES IN DATABASE "<database-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) warningVerify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method‚Äã This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets‚Äã Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: GRANT USAGE ON DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT USAGE ON ALL SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL MATERIALIZED VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT MONITOR ON PIPE "<pipe-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: GRANT USAGE ON ALL FUNCTIONS IN DATABASE "<database-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: GRANT OWNERSHIP ON FUNCTION <schema_name>.<udf_name> TO ROLE <role_name>; Replace the placeholders with the appropriate values: <schema_name>: The name of the schema that contains the user-defined function (UDF). <udf_name>: The name of the secure UDF that requires ownership permissions. <role_name>: The role that gets assigned ownership of the secure UDF. Did you know?The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets‚Äã To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT MONITOR ON FUTURE PIPES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT USAGE ON FUTURE FUNCTIONS IN DATABASE "<database-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) warningFor any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation. To grant permissions at a schema level: GRANT REFERENCES ON FUTURE TABLES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE VIEWS IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE STREAMS IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT MONITOR ON FUTURE PIPES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role; Replace <database-name> with the database and <schema-name> with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage‚Äã To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: USE ROLE ACCOUNTADMIN;GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role; To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: GRANT USAGE ON DATABASE "<cloned-database>" TO ROLE atlan_user_role;GRANT USAGE ON SCHEMA "<cloned-database>"."<cloned-account-usage-schema>" TO ROLE atlan_user_role;GRANT SELECT ON ALL TABLES IN SCHEMA "<cloned-database>"."<cloned-account-usage-schema>" TO ROLE atlan_user_role;GRANT SELECT ON ALL VIEWS IN SCHEMA "<cloned-database>"."<cloned-account-usage-schema>" TO ROLE atlan_user_role; Replace <cloned-database> with the name of the cloned database, and <cloned-account-usage-schema> with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets‚Äã To query and preview data within assets that already exist in Snowflake, add these permissions: GRANT USAGE ON DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT USAGE ON ALL SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON ALL STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT MONITOR ON PIPE "<pipe-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets‚Äã To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE STREAMS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT MONITOR ON FUTURE PIPES IN DATABASE "<database-name>" TO ROLE atlan_user_role; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) warningFor any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation. To grant permissions at a schema level: GRANT SELECT ON FUTURE TABLES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE VIEWS IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT SELECT ON FUTURE STREAMS IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role;GRANT MONITOR ON FUTURE PIPES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role; Replace <database-name> with the database and <schema-name> with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) warningVerify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags‚Äã Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher. To import tags from Snowflake, grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: USE ROLE ACCOUNTADMIN;GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role; The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: GRANT USAGE ON DATABASE "<copied-database>" TO ROLE atlan_user_role;GRANT USAGE ON SCHEMA "<copied-schema>" IN DATABASE "<copied-database>" TO ROLE atlan_user_role;GRANT REFERENCES ON ALL VIEWS IN DATABASE "<copied-database>" TO ROLE atlan_user_role; Replace <copied-database> with the copied Snowflake database name. Replace <copied-schema> with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake‚Äã To push tags updated for assets in Atlan to Snowflake, grant these permissions: GRANT APPLY TAG ON ACCOUNT TO ROLE <role-name>; You can learn more about tag privileges from Snowflake documentation. (Optional) To crawl dynamic tables‚Äã Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: GRANT MONITOR ON ALL DYNAMIC TABLES IN DATABASE "<DATABASE_NAME>" TO ROLE atlan_user_role; Grant permissions at a schema level: GRANT MONITOR ON ALL DYNAMIC TABLES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role; To crawl future dynamic tables from Snowflake: Grant permissions at a database level: GRANT MONITOR ON FUTURE DYNAMIC TABLES IN DATABASE "<DATABASE_NAME>" TO ROLE atlan_user_role; Grant permissions at a schema level: GRANT MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA "<database-name>.<schema-name>" TO ROLE atlan_user_role; Replace <database-name> with the database and <schema-name> with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables‚Äã Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: GRANT REFERENCES ON ALL ICEBERG TABLES IN DATABASE <database-name> TO ROLE atlan_user_role; To crawl future Iceberg tables in Snowflake: GRANT REFERENCES ON FUTURE ICEBERG TABLES IN DATABASE <database-name> TO ROLE atlan_user_role; To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: GRANT USAGE ON INTEGRATION <integration-name> TO ROLE atlan_user_role; warningYou must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages‚Äã Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: GRANT USAGE ON ALL STAGES IN DATABASE <database_name> TO ROLE atlan_user_role;GRANT READ ON ALL STAGES IN DATABASE <database_name> TO ROLE atlan_user_role; Replace <database_name> with the name of your Snowflake database Replace <atlan_user_role> with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: GRANT USAGE ON FUTURE STAGES IN DATABASE <database_name> TO ROLE atlan_user_role;GRANT READ ON FUTURE STAGES IN DATABASE <database_name> TO ROLE atlan_user_role; Replace <database_name> with the name of your Snowflake database Replace <atlan_user_role> with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP‚Äã If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request. (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.)Tags:connectorsdatacrawlPreviousSnowflakeNextSet up an AWS private network link to SnowflakeCreate user and role in SnowflakeChoose metadata fetching methodGrant permissions for account usage methodGrant permissions for information schema methodAllowlist the Atlan IP
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks
Title: Set up Databricks | Atlan Documentation
Content: Connect dataData WarehousesDatabricksGet StartedSet up DatabricksOn this pageSet up DatabricksAtlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication‚Äã Who can do this?Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace‚Äã To grant workspace access to the user creating a personal access token: From the left  of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions. In the Add permissions dialog, enter the following details: For User, group, or service principal, select the user to grant access. For Permission, click the dropdown and select workspace User. Generate a personal access token‚Äã You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan. To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings. Under the Settings , click Developer. On the Developer page, next to Access tokens, click Manage. On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment, enter a description of the token's intended use - for example, Atlan crawler. For Lifetime (days), consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important!If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate. Copy and save the generated token in a secure location, and then click Done. Select a cluster‚Äã Did you know?Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: Interactive cluster SQL warehouse (formerly SQL endpoint) Interactive cluster‚Äã To confirm an all-purpose interactive cluster is configured: From the left  of any page of your Databricks instance, click Compute. Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after ... minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname, Port, and HTTP Path. SQL warehouse (formerly SQL endpoint)‚Äã To confirm a SQL warehouse is configured: From the left  of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL. From the refreshed left , click SQL Warehouses. Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname, Port, and HTTP path. AWS service principal authentication‚Äã Who can do this?You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID Client secret Create a service principal‚Äã You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. Identity federation enabled‚Äã To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left  of the account console, click User management. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal. On the Add service principal page, enter a name for the service principal and then click Add. Once the service principal has been created, you can assign it to your identity federated workspace. From the left  of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions. In the Add permissions dialog, enter the following details: For User, group, or service principal, select the service principal you created. For Permission, click the dropdown and select workspace User. Identity federation disabled‚Äã To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings. In the left  of the Settings page, under the Workspace admin subheading, click Identity and access. On the Identity and access page, under Management and permissions, next to Service principals, click Manage. In the upper right of the Service principals page, click Add service principal. In the Add service principal dialog, click the Add new button. For New service principal display name, enter a name for the service principal and then click Add. Create an OAuth secret for the service principal‚Äã You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal: Log in to your Databricks account console as an account admin. From the left  of the account console, click User management. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created. On the service principal page, under OAuth secrets, click Generate secret. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Important!Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done. Azure service principal authentication‚Äã Who can do this?You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Client secret Tenant ID (directory ID) Create a service principal‚Äã To use service principals on Azure Databricks, an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal. If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top  to switch to the directory in which you want to create the service principal. In_Search resources, services, and docs_, search for and selectMicrosoft Entra ID. Click**+ Addand selectApp registration**. For_Name_, enter a name for the application. In the_Supported account types_section, selectAccounts in this organizational directory only (Single tenant) and then click Register. On the application page's_Overview_page, in the_Essentials_section, copy and store the following values in a secure location: Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, clickCertificates & secrets. On the_Client secrets_tab, clickNew client secret. In the_Add a client secret_dialog, enter the following details: For Description, enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add. Copy and store the client secret's_Value_in a secure place. Add a service principal to your account‚Äã To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left  of the account console, click User management. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal. On the Add service principal page, enter a name for the service principal. Under UUID, paste the Application (client) ID for the service principal. Click Add. Assign a service principal to a workspace‚Äã To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. Identity federation enabled‚Äã To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left  of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions. In the Add permissions dialog, enter the following details: For User, group, or service principal, select the service principal you created. For Permission, click the dropdown to select workspace User. Identity federation disabled‚Äã To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings. In the left  of the Settings page, under the Workspace admin subheading, click Identity and access. On the Identity and access page, under Management and permissions, next to Service principals, click Manage. In the upper right of the Service principals page, click Add service principal. In the Add service principal dialog, click the Add new button. For New service principal display name, paste the Application (client) ID for the service principal, enter a display name, and then click Add. Grant permissions to crawl metadata‚Äã You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege, currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG, USE SCHEMA, EXECUTE, READ VOLUME, and SELECT. To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left  of your workspace, click Catalog. In the left  of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals, click the dropdown and then select the user or service principal. Under Privileges, check the BROWSE privilege. At the bottom of the dialog, click Grant. (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method‚Äã To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction Public Preview‚Äã To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data‚Äã Important!Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left  of your workspace, click Catalog. In the left  of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals, click the dropdown and then select the user or service principal. Under Privilege presets, click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG, USE SCHEMA, EXECUTE, READ VOLUME, and SELECT. At the bottom of the dialog, click Grant. (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags‚Äã To import Databricks tags, you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token, an AWS service principal, or an Azure service principal, you need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMAon system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges: APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables‚Äã You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables: lineage usage and popularity metrics Enable system.access schema‚Äã You need your account admin to enable the system.access schema using the SystemSchemas API. This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation: List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted. This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted. (Optional) enable system.information_schema.table‚Äã To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. Grant permissions‚Äã Who can do this?You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main) and then to the appropriate schema (for example, sales). Click the Permissions tab. Click Grant. Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema‚Äã This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API. This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation. If enabled for any given schema, the state is EnableCompleted. infoüí™ Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables. Grant permissions‚Äã Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods. Once you have created a personal access token, an AWS service principal, or an Azure service principal, you need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history(to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. infoüí™ Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables. (Optional) Create cloned views of system tables‚Äã When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names‚Äîfor example, atlan_cloned_catalog and atlan_cloned_schema. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW <cloned-catalog-name>.<cloned-schema-name>.column_lineage ASSELECT * FROM system.access.column_lineage;CREATE OR REPLACE VIEW <cloned-catalog-name>.<cloned-schema-name>.table_lineage ASSELECT * FROM system.access.table_lineage; Replace <cloned-catalog-name> and <cloned-schema-name> with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW <cloned-catalog-name>.<cloned-schema-name>.query_history ASSELECT * FROM system.query.history; Replace <cloned-catalog-name> and <cloned-schema-name> with the catalog and schema names used in your environment. Grant permissions‚Äã Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, <cloned-catalog-name>) USE SCHEMA and SELECT on the schema (for example, <cloned-catalog-name>.<cloned-schema-name>) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID‚Äã To extract lineage and usage and popularity metrics using system tables, you also need the warehouse ID of your SQL warehouse. To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left  of your workspace, click SQL Warehouses. On the Compute page, select the warehouse you want to use. From the Overview tab of your warehouse page, next to the Name of your warehouse, copy the value for your SQL warehouse ID. For example, example-warehouse (ID: 123ab4c5def67890), copy the value 123ab4c5def67890 and store it in a secure location. (Optional) Grant view permissions to access Databricks entities via APIs‚Äã Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API(/api/2.0/workspace/list): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders. Queries API(/api/2.0/sql/queries): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries. Job API(/api/2.2/jobs/list): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job. Pipeline API(/api/2.0/pipelines): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions. (Optional) Grant permissions for views and materialized views‚Äã Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left  of your workspace, click Catalog. In the Catalog Explorer, select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant. In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals. Select the following privileges under Privilege presets: USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3‚Äì6 for each catalog you want to crawl in Atlan. Did you know?SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history‚Äã To mine query history using REST API, you need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left  of your workspace, click SQL Warehouses. On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions. In the Manage permissions dialog, configure the following: In the Type to add multiple users or groups field, search for and select a user or service principal. Expand the Can use permissions dropdown and then select Can manage. This permission enables the service principal to view all queries for the warehouse. Click Add to assign the CAN MANAGE permission to the service principal. Tags:dataauthenticationPreviousDatabricksNextSet up cross-workspace extractionPersonal access token authenticationAWS service principal authenticationAzure service principal authenticationGrant permissions to crawl metadata(Optional) Grant permissions to query and preview data(Optional) Grant permissions to import and update tags(Optional) Grant permissions to extract lineage and usage from system tables(Optional) Grant view permissions to access Databricks entities via APIs(Optional) Grant permissions for views and materialized views(Optional) Grant permissions to mine query history
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi
Title: Set up Microsoft Power BI | Atlan Documentation
Content: Connect dataBI ToolsOn-premises & Enterprise BIMicrosoft Power BIGet StartedSet up Microsoft Power BIOn this pageSet up Microsoft Power BIWho can do this?Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator (formerly known as Power BI Administrator) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin‚Äã Register application in Microsoft Entra ID‚Äã Who can do this?You need your Cloud Application Administrator or Application Administrator to complete these steps‚Äî> you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal. Search for Microsoft Entra ID and select it. Click App registrations from the left . Click + New registration. Enter a name for your client application and click Register. From the Overview screen, copy and securely store: Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left . Under Client secrets, click + New client secret. Enter a description, select an expiry time, and click Add. Copy and securely store the client secret Value. Create security group in Microsoft Entra ID‚Äã Who can do this?You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal. Search for Microsoft Entra ID and select it. Click Groups under the Manage section. Click New group. Set the Group type to Security. Enter a Group name and optional description. Click No members selected. Add the appropriate member: For Delegated User authentication: search for the user and select it. For Service Principal authentication: search for the application registration created earlier and select it. Click Select and then Create. By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options‚Äã Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended)‚Äã When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: Admin API only‚Äã This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. Who can do this?You need your Fabric Administrator (formerly known as Power BI Administrator) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal. Click Tenant settings under Admin portal. Under Admin API settings: Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Admin and non-admin APIs‚Äã This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. Assign security group to Power BI workspaces in PowerBI service portal‚Äã Who can do this?You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage. Open Workspaces and select the workspace you want to access from Atlan. Click Access. In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer: For workspaces without parameters Contributor: For workspaces with semantic models containing parameters or to generate lineage for measures Member: To generate lineage for dataflows Click Add. Configure admin and non-admin API access in PowerBI Service Portal‚Äã Who can do this?You need your Fabric Administrator (formerly known as Power BI Administrator) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal. Click Tenant settings under Admin portal. Under Developer settings: Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Under Admin API settings: Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication‚Äã infoAtlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. Fabric administrator role assignment‚Äã Who can do this?You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal. Click Users and then Active users from the left . Select the delegated user. Under Roles, click Manage roles. Expand Show all by category. Under Collaboration, select Fabric Administrator. Click Save changes. API permissions‚Äã Who can do this?You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. warningThe following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal‚Äîit's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application: In your app registration, click API permissions under the Manage section. Click Add a permission. Search for and select Power BI Service. Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). Admin API settings configuration‚Äã Who can do this?You need your Fabric Administrator (formerly known as Power BI Administrator) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal. Click Tenant settings under Admin portal. Under Admin API settings: Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply. Tags:dataauthenticationPreviousMicrosoft Power BINextCrawl Microsoft Power BIBefore you beginConfigure authentication options
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/platform/references/atlan-architecture
Title: Atlan architecture | Atlan Documentation
Content: Get StartedReferencesAtlan architectureOn this pageAtlan architectureAtlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS)‚Äã Microsoft Azure‚Äã Google Cloud Platform (GCP)‚Äã The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components‚Äã Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Metastore stores metadata as data in a graph store. It is based on Apache Atlas and has fine-grained access control on top. Apache Zookeeper manages consensus and coordination for the metastore services. Elasticsearch indexes data and drives search functionality. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite. Redis is a cache layer used by Heracles. Platform management components‚Äã Velero performs cluster backups. Kibana explores and filters log data stored in Elasticsearch. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Elasticsearch stores and indexes logs. Central components‚Äã Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured)‚Äã The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more.Tags:securityaccess-controlpermissionsPreviousIncident response planNextProduct release stagesPlatform componentsPlatform management componentsCentral componentsAtlan marketplace (not pictured)
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension
Title: Use the Atlan browser extension | Atlan Documentation
Content: Configure AtlanIntegrationsAutomationBrowser ExtensionHow-tosHow to use the Atlan browser extensionOn this pageUse the Atlan browser extensionThe Atlan browser extension provides metadata context directly in your supported data tools. You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension‚Äã To install the Atlan browser extension, first log into your Atlan instance. Atlan saves your Atlan domain in a cookie when you log in. To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https://chrome.google.com/webstore/detail/atlan/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile. Click the four dots icon in the resulting dialog to get to integrations. Under Apps, for Browser extension, click Install. To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome. When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store. Currently, you can't install the browser extension on mobile devices or tablets. Did you know?You can also install Atlan's browser extension at the workspace level. To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers. Configure the extension‚Äã Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources, if any. Configure the extension as a user‚Äã To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com), the Atlan instance URL appears preselected. Click Get started. (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com), enter the URL of your Atlan instance and then click Get started. After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources, you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! üéâ (Optional) Configure custom domains as an admin‚Äã Who can do this?You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left  of any screen, click Admin. Under Workspace, click Integrations. Under Apps, expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source..., if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector, select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. infoüí™ Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team, you can either install the Atlan browser extension for your own use or share the link with your users. Usage‚Äã Who can do this?Anyone with access to Atlan‚Äîany admin, member, or guest user‚Äîand a supported tool can use the browser extension. First, log into Atlan. Did you know?When using Atlan's browser extension in a supported tool, the extension only reads the URL of your browser tab‚Äîno other data is accessed. If using Atlan's browser extension on any website, it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security. Access and enrich context in-flow‚Äã To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. warningThe icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! üéâ The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Did you know?Your permissions in Atlan control what metadata you can see and change in the extension. Search for metadata‚Äã To search for context for any information on any website: Select the text you'd like to search on the web page you're viewing. Right-click, and then select Search in Atlan üè°. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! üéâ Add a resource‚Äã You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension. In the resource clipper , under Link this page to an asset, select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! üéâ Did you know?The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools‚Äã Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight: analyses, dashboards, and datasets Databricks: databases, schemas, views, and tables dbt Cloud: models and sources in the model editor and dbt docs Google BigQuery: datasets, schemas, views, and tables IBM Cognos Analytics: folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker: dashboards, explores, and folders Microsoft Power BI: dashboards, reports, dataflows, and datasets Mode: collections, reports, queries, and charts Qlik Sense Cloud: apps, datasets, sheets, and spaces Redash: queries, dashboards, and visualizations Salesforce: objects Sigma: datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau: dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot: liveboards, answers, visualizations, and tables MicroStrategy: dossiers, reports, documents Tags:atlandocumentationPreviousConfigure the extension for managed browsersNextEnable embedded metadata in TableauInstall the extensionConfigure the extensionUsageSupported tools
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/get-started/how-tos/quick-start-for-admins
Title: Administrators | Atlan Documentation
Content: Get StartedQuick Start GuidesAdministratorsOn this pageAdministratorsUser management‚Äã User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center‚Äã It's super simple to invite and remove users from Atlan from the Admin center. You can also manage existing users by adding them to groups, changing their roles, or set up SSO, SCIM, and SMTP configurations. Manage access control from the governance center‚Äã The Governance center is where you can build access control mechanisms to manage user access. Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers, and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile‚Äã The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. Glossary‚Äã The Atlan glossary is a rich tool for defining and organizing your data terminology to improve transparency and share knowledge. No need to ask around for what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one searchable place. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Did you know?The glossary helps power Atlan's powerful search tool, so tagging and defining assets are critical to helping your team find what they need. Discovery‚Äã We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms and descriptions you've added to your data assets. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan - saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Tags:get-startedquick-startPreviousWhat is Atlan?NextData consumersUser managementAsset profileGlossaryDiscovery
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/secure-agent
Title: Secure Agent | Atlan Documentation
Content: Connect dataSecure AgentOn this pageSecure AgentThe Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn‚Äôt require inbound connectivity. Running within an organization‚Äôs controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities‚Äã The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture‚Äã Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction‚Äã A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment‚Äã Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations‚Äã Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works‚Äã The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also‚Äã Deployment architecture: Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction.Tags:securityaccess-controlpermissionsNextInstall on Virtual Machine (K3s)Key capabilitiesHow it worksSee also
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/capabilities/playbooks
Title: Playbooks | Atlan Documentation
Content: Configure AtlanPlaybooksOn this pagePlaybooks Overview: Atlan's playbooks provide reusable workflows and automation for common data tasks. Create, share, and execute standardized processes to maintain consistency, reduce manual effort, and enable self-service for data consumers while following governance standards. Get started‚Äã How to set up playbooks Guides‚Äã Playbook management‚Äã How to manage playbooks: Monitor and maintain your playbook workflows. How to automate data profiling: Set up automated data quality checks. Troubleshooting‚Äã Troubleshooting playbooks: Solutions for common playbook issues. Tags:playbooksautomationworkflowsmetadatacapabilitiesNextSet up playbooksGet startedGuidesTroubleshooting
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/capabilities/discovery
Title: Discovery | Atlan Documentation
Content: Use dataDiscoveryOn this pageDiscovery Overview: Atlan's discovery capabilities help users find, understand, and use data assets across your organization. With powerful search, filtering, and browsing features, users can quickly locate relevant data assets, explore their context, and access the information they need to make data-driven decisions. Get started‚Äã How to search and discover assets For detailed search, filtering, and troubleshooting information, use the sidebar . Tags:discoverysearchbrowsecapabilitiesNextSearch and discover assetsGet started
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/capabilities/governance/contracts
Title: Contracts | Atlan Documentation
Content: Build governanceContractsOn this pageContracts Overview: Manage data contracts and agreements in Atlan to ensure data quality and compliance. Define and track data quality expectations, service level agreements (SLAs), and data sharing agreements between teams and systems. Get started‚Äã Follow these steps to implement contracts in Atlan: Create data contracts Guides‚Äã Add contract impact analysis in GitHub: Detailed instructions on adding contracts for impact analysis in GitHub. Tags:contractsagreementsdata qualitygovernanceatlanNextCreate data contractsGet startedGuides
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/integrations
Title: Integrations | Atlan Documentation
Content: Configure AtlanIntegrationsIntegrations Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts‚Äã Integration categories: Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods: Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync: Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks: Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings‚Äã ‚öôÔ∏èAutomationConnect with platforms like AWS Lambda to automate data workflows and streamline routine tasks.üë•CollaborationIntegrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing.üí¨CommunicationConnect with SMTP for real-time alerts.üîêIdentity managementIntegrate with identity providers like Okta and Azure AD for seamless authentication and user management.üìãProject managementConnect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started‚Äã 1Select an integrationChoose from Atlan's available integrations based on your team's tools and workflows.‚Üí2Configure connectionFollow the integration-specific setup guide to establish a secure connection with your tool.‚Üí3Test and activateVerify the integration is working correctly with a test action, then activate for your organization. üí°Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack.Tags:integrationsatlansetupNextAutomation Integrations
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/support/references/customer-support
Title: Customer support | Atlan Documentation
Content: SupportReferencesCustomer supportOn this pageCustomer supportOne of Atlan's core values is to help you and your team do your life's best work. üíô That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps/engineering support personnel Vast repository of self-service resources Service-level commitment‚Äã Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support‚Äã ‚úâÔ∏è Email support at a dedicated customer support email account ([email protected]) üë®‚Äçüíª In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. üìù Submit a support request via the online form. To track your support tickets: Navigate to https://atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities. On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation‚Äã 24x7 availability for all requests and issues Severity levels‚Äã The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: SeverityDescriptionBasic supportAdvanced supportS0Production software is unavailable; all customers are blocked and productivity halted2 hours1 hourS1Production software is available; functionality or performance is severely impaired4 hours2 hoursS2Production software is available and usable with partial, noncritical loss of functionality. Or, production software has an occasional issue that customer requests identification and resolution. Also includes requests for help with administrative tasks16 hours4 hoursS3Cosmetic issues or request for general information about the software, documentation, processes, or procedures24 hours14 hours Escalation procedure‚Äã If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation: Technical Support Engineer 2nd level of escalation: Director, Support 3rd level of escalation: Head of Customer Experience Tags:supportService-level commitmentWays to contact supportHours of operationSeverity levelsEscalation procedure
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake
Title: Snowflake | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeOn this pageSnowflake Overview: Catalog Snowflake databases, schemas, tables, and views in Atlan. Gain visibility into lineage, usage, and governance for your cloud data warehouse assets. Get started‚Äã Follow these steps to connect and catalog Snowflake assets in Atlan: Set up the connector Crawl Snowflake assets Guides‚Äã Authentication‚Äã Enable Snowflake OAuth: Set up OAuth authentication for Snowflake connections. Metadata & lineage‚Äã Mine Snowflake: Learn how to mine query history and construct lineage for Snowflake assets. Tag management‚Äã Manage Snowflake tags: Configure and manage tags and policy tags in Snowflake. Advanced features‚Äã Configure Snowflake data metric functions: Configure and use data metric functions in Snowflake. Private networking‚Äã Set up an AWS private network link to Snowflake: Establish a secure, private network connection to Snowflake on AWS. Set up an Azure private network link to Snowflake: Establish a secure, private network connection to Snowflake on Azure. References‚Äã What does Atlan crawl from Snowflake: Learn about the Snowflake assets and metadata that Atlan discovers and catalogs. Preflight checks for Snowflake: Verify prerequisites before setting up the Snowflake connector. Troubleshooting‚Äã Troubleshooting connectivity: Resolve common Snowflake connection issues and errors. Best practices‚Äã Snowflake warehouse configuration: Recommended Snowflake warehouse configuration to enable reliable Atlan workflow execution. Tags:connectivitysnowflakeNextSet up SnowflakeGet startedGuidesReferencesTroubleshootingBest practices
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake
Title: Crawl Snowflake | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeCrawl Snowflake AssetsCrawl SnowflakeOn this pageCrawl SnowflakeOnce you have configured the Snowflake user permissions, you can establish a connection between Atlan and Snowflake. (If you are also using AWS PrivateLink or Azure Private Link for Snowflake, you will need to set that up first, too.) To crawl metadata from Snowflake, review the order of operations and then complete the following steps. Select the source‚Äã To select Snowflake as your source: In the top right of any screen, navigate to New and then click New Workflow. From the list of packages, select Snowflake Assets and click on Setup Workflow. Provide credentials‚Äã Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3. This is currently only supported when using the information schema extraction method to fetch metadata with basic authentication. In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method‚Äã To enter your Snowflake credentials: For Account Identifiers (Host), enter the hostname, AWS PrivateLink endpoint, or Azure Private Link endpoint for your Snowflake instance. For Authentication, choose the method you configured when setting up the Snowflake user: For Basic authentication, enter the Username and Password you configured in either Snowflake or the identity provider. infoüí™ Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. For Keypair authentication, enter the Username, Encrypted Private Key, and Private Key Password you configured. Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to Snowflake documentation. For Okta SSO authentication, enter the Username, Password, and Authenticator you configured. The Authenticator will be the Okta URL endpoint of your Okta account, typically in the form of https://<okta_account_name>.okta.com. For Role, select the Snowflake role through which the crawler should run. For Warehouse, select the Snowflake warehouse in which the crawler should run. Click Test Authentication to confirm connectivity to Snowflake using these details. Once successful, at the bottom of the screen, click Next. Offline extraction method‚Äã Atlan supports the offline extraction method for fetching metadata from Snowflake. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3. To enter your S3 details: For Bucket name, enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank. For Bucket prefix, enter the S3 prefix under which all the metadata files exist. These include databases.json, columns-<database>.json, and so on. For Bucket region, enter the name of the S3 region. When complete, at the bottom of the screen, click Next. Configure the connection‚Äã To complete the Snowflake connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production, development, gold, or analytics. (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins. warningIf you do not specify any user or group, nobody will be able to manage the connection - not even admins. (Optional) To prevent users from querying any Snowflake data, change Allow SQL Query to No. (Optional) To prevent users from previewing any Snowflake data, change Allow Data Preview to No. At the bottom of the screen, click Next to proceed. Agent extraction method‚Äã Atlan supports using a Secure Agent for fetching metadata from Snowflake. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the crawler‚Äã warningWhen modifying an existing Snowflake connection, switching to a different extraction method will delete and recreate all assets in the existing connection. If you'd like to change the extraction method, contact Atlan support for assistance. Before running the Snowflake crawler, you can further configure it. You must select the Extraction method you configured when you set up Snowflake: For Information Schema method, keep the default selection. Change to Account Usage method and specify the following: Database Name of the copied Snowflake database Schema Name of the copied ACCOUNT_USAGE schema Incremental extraction Public preview - Toggle incremental extraction for faster and more efficient metadata extraction. You can override the defaults for any of the remaining options: For Asset selection, select a filtering option: To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This will default to all assets, if none are specified.) To have the crawler include Databases, Schemas, or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases will include all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This will default to no assets, if none are specified.) To have the crawler ignore Databases, Schemas, or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views will exclude all the matching tables and views. Click + to add more filters. If you add multiple filters, assets will be crawled based on matching all the filtering conditions you have set. To exclude lineage for views in Snowflake, change View Definition Lineage to No. To import tags from Snowflake to Atlan, change Import Tags to Yes. Note the following: If using the Account Usage extraction method, grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. If using the Information Schema extraction method, note that Snowflake stores all tag objects in the ACCOUNT_USAGE schema. You will need to grant permissions on the account usage schema instead to import tags from Snowflake. warningObject tagging in Snowflake currently requires Enterprise Edition or higher. If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error - unable to retrieve tags. For Control Config, keep Default for the default configuration or click Custom to further configure the crawler: If you have received a custom crawler configuration from Atlan support, for Custom Config, enter the value provided. You can also: Enter {"ignore-all-case": true} to enable crawling assets with case-sensitive identifiers. For Enable Source Level Filtering, click True to enable schema-level filtering at source or keep False to disable it. For Use JDBC Internal Methods, click True to enable JDBC internal methods for data extraction or click False to disable it. For Exclude tables with empty data, change to Yes to exclude any tables and corresponding columns without any data. For Exclude views, change to Yes to exclude all views from crawling. Did you know?If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler‚Äã To run the Snowflake crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks. You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! üéâ Note that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run.Tags:connectorsdatacrawlPreviousHow to enable Snowflake OAuthNextMine SnowflakeSelect the sourceProvide credentialsConfigure the connectionConfigure the crawlerRun the crawler
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/connections/how-tos/manage-connectivity
Title: Manage connectivity | Atlan Documentation
Content: Connect dataConnectivity FrameworkConnector FrameworkManage workflowsManage connectivityOn this pageManage connectivityOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Modify connectivity‚Äã To modify the configuration of an existing workflow, complete the following steps. On the left of any screen, navigate to Workflow. Under Monitor select an existing workflow tile. (You may need to expand the run history or filter first.) From the Workflow Run History table, click on the previous run of the workflow you want to modify. In the upper left of the screen, change to the Config tab. Modify the parts of the workflow configuration you require: Under <Connector> Credential, use the Edit Credentials button to change the credentials for the source. warningIf you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically. Under Connection settings, use the Edit button to change the connection details: Modify whether or not querying or data previews are allowed for the source. Modify the query row limit to enable exporting large query results via email. Modify the query timeout limit - expandable up to 60 minutes. Under Connection Admins, click the pencil icon to add or remove connection admins. warningIf you do not specify any user or group, nobody will be able to manage the connection - not even admins. Under Metadata, use the selectors to modify which metadata to include and exclude. To check for any permissions or other configuration issues before running the workflow, click Preflight checks. Once you've made your updates, click the Update button to save the changes. You can optionally run the workflow with the new configuration immediately. You will need to confirm your changes by clicking the Yes button. Note that some workflow changes may take a few minutes to come into effect. That's it - next time you run the workflow, or it runs on its schedule, it will use your changes! üéâ warningIf you modify the Metadata portion, any previously crawled metadata that is now excluded will be archived on the next workflow run.Tags:integrationconnectorsworkflowautomationorchestrationPreviousWhat is the crawler logic for a deprecated asset?NextMonitor connectivityModify connectivity
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity
Title: Troubleshooting Snowflake connectivity | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeTroubleshootingTroubleshooting Snowflake connectivityOn this pageTroubleshooting Snowflake connectivityHow to debug test authentication and preflight check errors?‚Äã Missing warehouse grants The user doesn‚Äôt have USAGE and OPERATE grants on a warehouse. Grant warehouse access to the role: GRANT OPERATE, USAGE ON WAREHOUSE "<warehouse>" TO ROLE atlan_user_role; Then, ensure that you grant the role to the new user: GRANT ROLE atlan_user_role TO USER atlan_user; Missing authorized access to SNOWFLAKE.ACCOUNT_USAGE schema The user doesn‚Äôt have authorized access to the SNOWFLAKE.ACCOUNT_USAGE database Reach out to your account admin to grant imported privileges on the Snowflake database to the role: USE ROLE ACCOUNTADMIN;GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role; If using a copied database, you'll need to grant the following permissions: GRANT USAGE ON DATABASE "<copied-database>" TO ROLE atlan_user_role;GRANT USAGE ON SCHEMA "<copied-schema>" IN DATABASE "<copied-database>" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE "<copied-database>" TO ROLE atlan_user_role; Missing usage grants on databases and/or schemas The user doesn't have usage grants to the databases ` $missingDatabases ` and schemas ` $missingSchemas Grant missing permissions listed here for information schema extraction method. Atlan IP not allowlisted Atlan's current location or network isn't recognized by Snowflake's security settings. This can happen if Atlan's IP address isn't on the list of allowed addresses in Snowflake's network policies. If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Contact Atlan support to obtain Atlan's IP addresses. Incorrect credentials The username or the password provided to connect to the Snowflake account is incorrect. Sign into the Snowflake account for the specified host and verify that the username and password are correct. You can also create a new user, if required, by following the steps here. Missing or unauthorized role The role specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this role. If the role does not exist or is missing the required grants, create a role and then grant the role to the user. User account locked The user account you're using to connect to Snowflake has been locked temporarily because of multiple incorrect login attempts. Wait for the user account to unlock or create a different user account to continue. Missing or unauthorized warehouse The warehouse specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this warehouse. Ensure that the warehouse name is configured correctly. Update the warehouse name in the configuration if your account is using a different warehouse. Create a role and then grant the role to the user for the updated warehouse. Missing access to non-system databases or schemas The configured user doesn't have usage grants to any database or schema. or The configured user doesn't have usage grants to any non-system database or schema. This pertains to the information schema method of fetching metadata. Ensure that the user has authorized access to the databases and schemas to be crawled. Grant the requisite permissions as outlined here. Why are some assets from a database or schema missing?‚Äã Check the grants on the role attached to the user defined for the crawler. Ensure the missing database or schema is present in these grants. SHOW GRANTS TO ROLE atlan_user_role; Why are new tables or views missing?‚Äã When using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata. Make sure the role attached to the user defined for the crawler has grants for future tables and views being created in the database: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE VIEWS IN DATABASE "<database-name>" TO ROLE atlan_user_role;GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE "<database-name>" TO ROLE atlan_user_role; Make sure you run the below commands as well so that new tables and views you've created in-between are also visible to the user: GRANT USAGE ON ALL SCHEMAS IN DATABASE "<database-name>" TO role atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE "<database-name>" TO role atlan_user_role; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE "<database-name>" TO atlan_user_role;GRANT REFERENCES ON ALL VIEWS IN DATABASE "<database-name>" TO role atlan_user_role; Why is some lineage missing?‚Äã The query miner only mines query history for up to the previous two weeks. The miner will not mine any queries that ran before that time window. If the queries that created your assets ran before that time window, lineage for those assets will not be present. To mine more than the previous two weeks of query history, either use S3-based query mining or contact Atlan support. Note that Snowflake itself only retains query history for so long as well, though. Once Snowflake itself no longer contains the query history we will be unable to mine it for lineage. Lineage is unsupported for parameterized queries. Snowflake currently does not resolve values for parameterized queries before logging them in query history. This limits Atlan from generating lineage in such cases. Missing attributes and lineage‚Äã When using the account usage extraction method, there are currently some limitations. We are working with Snowflake to find workarounds for crawling the following: External table location data Procedures Primary key designation Furthermore, only database-level filtering is currently possible. What views does Atlan require access to for the account usage method?‚Äã When using the account usage method for fetching metadata, Atlan requires access to the following views in Snowflake: For the crawler: DATABASES, SCHEMATA, TABLES, VIEWS, COLUMNS, and PIPES For the miner and popularity metrics: QUERY_HISTORY, ACCESS_HISTORY, and SESSIONS Why am I getting a destination URL mismatch error when authenticating via Okta SSO?‚Äã This error can occur when you're connecting to Snowflake through Okta SSO and enter the URL of your Snowflake instance in a format different from the one used in Okta. Snowflake follows two URL formats: Legacy format - <AccountLocator>.<Region>.snowflakecomputing.com or <AccountLocator>.<Region>.<cloud>.snowflakecomputing.com New URL format - <Orgname>-<AccountName>.snowflakecomputing.com Ensure that you're using the same Snowflake URL format in Snowflake and Okta. Refer to Snowflake documentation to learn more. Why am I getting a 'name or service not known' error when connecting via private link?‚Äã If you're getting the following error messages - java.net.UnknownHostException and Name or service not known - this is a known error for users who have upgraded to the Snowflake JDBC driver version 3.13.25., have underscores in their account name, and connect to their Snowflake accounts over private link (for example, https://my_account.us-west-2.privatelink.snowflakecomputing.com). If your Snowflake account name has an underscore - for example, my_account - the updated JDBC driver will automatically convert underscores to dashes or hyphens -. This does not affect normal URLs because Snowflake accepts URLs with both hyphens and underscores. For private link users, however, the JDBC driver will return an error if there are underscores present in the account name and the connection will fail. To troubleshoot further, refer to Snowflake documentation.Tags:atlandocumentationPreviousPreflight checks for SnowflakeNextTroubleshooting Snowflake tag management
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/mine-snowflake
Title: Mine Snowflake | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeCrawl Snowflake AssetsMine SnowflakeOn this pageMine SnowflakeOnce you have crawled assets from Snowflake, you can mine its query history to construct lineage. To mine lineage from Snowflake, review the order of operations and then complete the following steps. Select the miner‚Äã To select the Snowflake miner: In the top right of any screen, navigate to New and then click New Workflow. From the filters along the top, click Miner. From the list of packages, select Snowflake Miner and then click Setup Workflow. Configure the miner‚Äã To configure the Snowflake miner: For Connection, select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method, select Source, Agent, or see the separate instructions for the S3 miner. For Snowflake Database: If the connection is configured with access to the snowflake database, choose Default. If the connection can only access a separate cloned database, choose Cloned Database. If you are using a cloned database, enter the name of the cloned database in Database Name and the name of the cloned schema in Schema Name. For Start time, choose the earliest date from which to mine query history. infoüí™ Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. To check for any permissions or other configuration issues before running the miner, click Preflight checks. At the bottom of the screen, click Next to proceed. Agent extraction method‚Äã Atlan supports using a Secure Agent for mining query history from Snowflake. To use a Secure Agent, follow these steps: Important!Query redaction is a compute-intensive operation and can impact workflow performance. To use this feature effectively, assign at least 4 CPU cores to the agent. Select the Agent tab. Configure Query Redaction: By default, Query Redaction is turned off. You may choose to configure query redaction: Set Enable Query Redaction flag to true if you want to redact PII data from query history before ingesting it to Atlan. Configure PII Patterns for Query Redaction: Define patterns to identify and redact sensitive information using the following JSON format: { "pii_patterns": [ { "name": "email", "regex": "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b", "replacement": "[EMAIL]" } ]} Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the connection configuration used when crawling Snowflake. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Important!If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here. Configure the miner behavior‚Äã To configure the Snowflake miner behavior: (Optional) For Calculate popularity, keep True to retrieve usage and popularity metrics for your Snowflake assets from query history. For Excluded Users, type the names of users to be excluded while calculating usage metrics for Snowflake assets. Press Enter after each name to add more names. (Optional) For Advanced Config, keep Default for the default configuration or click Custom to configure the miner: If Atlan support has provided you with a custom control configuration, enter the configuration into the Custom Config box. You can also enter {‚Äúignore-all-case‚Äù: true} to enable crawling assets with case-sensitive identifiers. For Popularity Window (days), 90 days is the maximum limit. You can set a shorter popularity window of less than 90 days. Run the miner‚Äã To run the Snowflake miner, after completing the configuration steps: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you can see lineage for Snowflake assets that were created in Snowflake between the start time and when the miner ran! üéâTags:connectorsdatacrawlsetupPreviousCrawl SnowflakeNextManage Snowflake tagsSelect the minerConfigure the minerConfigure the miner behaviorRun the miner
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags
Title: Manage Snowflake tags | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeManage Snowflake in AtlanManage Snowflake tagsOn this pageManage Snowflake tagsNote that object tagging in Snowflake currently requires Enterprise Edition or higher. Atlan enables you to import your Snowflake tags, update your Snowflake assets with the imported tags, and push the tag updates back to Snowflake: Import tags - crawl Snowflake tags from Snowflake to Atlan Reverse sync - sync Snowflake tag updates from Atlan to Snowflake Once you've imported your Snowflake tags to Atlan: Your Snowflake assets in Atlan are automatically enriched with their Snowflake tags. Imported Snowflake tags are mapped to corresponding Atlan tags through case-insensitive name match - multiple Snowflake tags can be matched to a single tag in Atlan. You can also attach Snowflake tags, including tag values, to your Snowflake assets in Atlan - allowing you to categorize your assets at a more granular level. Atlan supports: Allowed values: attach an allowed value from a predefined list of values imported from Snowflake. Tag values: enter any value in Atlan while attaching or editing imported Snowflake tags on an asset. You can enable reverse sync to push any tag updates for your Snowflake assets back to Snowflake - including allowed and tag values added to assets in Atlan. You can filter your assets by Snowflake tags and tag and allowed values. Did you know?Enabling reverse sync only updates existing tags in Snowflake. It neither creates nor deletes any tags in Snowflake. Prerequisites‚Äã Did you know?Additional privileges are only required when using the information schema method for fetching metadata. This is because Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner, any permissions required are already set. Account usage method‚Äã Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. Information schema method‚Äã Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant additional permissions to import tags from Snowflake. Grant additional permissions to push updated tags to Snowflake. Import Snowflake tags to Atlan‚Äã Who can do this?You need to be an admin user in Atlan to import Snowflake tags to Atlan. You also need to work with your Snowflake administrator to grant additional permissions to import tags from Snowflake - you may not have access yourself. You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags are matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets are enriched with their synced tags from Snowflake. To import Snowflake tags to Atlan, you can either: Create a new Snowflake workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Snowflake workflow to change Import Tags to Yes. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan preserves those tags. Once the crawler has completed running, tags imported from Snowflake are available to use for tagging assets! üéâ View Snowflake tags in Atlan‚Äã Once you've imported your Snowflake tags, you can view and manage your Snowflake tags in Atlan. To view Snowflake tags: From the left  of any screen, click Governance. Under the Governance heading of the _Governance cente_r, click Tags. (Optional) Under Tags, click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. From the left  under Tags, select a synced tag - synced tags display the Snowflake ‚ùÑÔ∏è icon next to the tag name. In the Overview section, you can view a total count of synced Snowflake tags. To the right of Overview, click Synced tags to view additional details - including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Snowflake tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon. You can't rename tags synced from Snowflake. Push tag updates to Snowflake‚Äã Who can do this?Any admin or member user in Atlan can configure reverse sync for tag updates to Snowflake. You also need to work with your Snowflake administrator to grant additional permissions to push updates - you may not have access yourself. Did you know?Reverse sync is currently only available for imported Snowflake tags in Atlan. The imported tags display a Snowflake ‚ùÑÔ∏è icon next to the tag name. If using the account usage method, expect a data latency of up to 3 hours for reverse tag sync to be successful. You can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source. Once you have enabled reverse sync, any Snowflake assets with tags updated in Atlan are also updated in Snowflake. To enable reverse sync for imported Snowflake tags: From the left  of any screen, click Governance. Under the Governance heading of the _Governance cente_r, click Tags. (Optional) Under Tags, click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. In the left  under Tags, select a synced Snowflake tag - synced tags display the Snowflake ‚ùÑÔ∏è icon next to the tag name. On your selected tag page, to the right of Overview, click Synced tags. Under Synced tags, in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Snowflake. In the advanced settings, you can also enable concatenation to support multiple tag values for a single column. For detailed information about multiple tag values and concatenation, see Multiple tag values and concatenation. In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel. Now when you attach Snowflake tags to your Snowflake assets in Atlan, these tag updates are also pushed to Snowflake! üéâ Did you know?Enabling reverse sync won't trigger any updates in Snowflake until synced tags are attached to Snowflake assets in Atlan. For any questions about managing Snowflake tags, head over here.Tags:connectorsdatacrawlPreviousMine SnowflakeNextConfigure Snowflake data metric functionsPrerequisitesImport Snowflake tags to AtlanView Snowflake tags in AtlanPush tag updates to Snowflake
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/support/submit-request
Title: Submit support request | Atlan Documentation
Content: Submit requestAim to include as much information and detail in your request as possible to reduce delays between replies.Name*Email*Subject*Atlan URL*The URL of your Atlan tenantSeveritySEV0 (S0) - System down or critical issueSEV1 (S1) - Major functionality affectedSEV2 (S2) - General question or minor issueSEV3 (S3) - Feature request or enhancementRefer to severity and response SLA guidelines as outlined hereHow is this impacting you?I'm unable to use the productA major feature stopped workingAn issue is slowing me downI have a non-urgent questionI have a suggestion that will help me with my use-caseI need help with somethingDescription*AttachmentsüìÅAdd file or drop files hereSubmit
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/tags/connectors
Title: 300 docs tagged with "connectors" | Atlan Documentation
Content: 300 docs tagged with "connectors"View all tagsAdd impact analysis in GitHubLearn about add impact analysis in github.Add impact analysis in GitLabLearn about add impact analysis in gitlab.Atlan browser extension securityLearn about atlan browser extension security.Attach a tagAtlan allows users to add [tags](/product/capabilities/governance/tags/concepts/what-are-tags) to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection.Automate data profiling‚ûïAvailable via the Data Quality Studio packageBulk enrich metadataAtlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan.Can Atlan integrate with Airflow to generate lineage?Atlan currently supports native integration with [Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage).Can I add Atlan's browser extension for everyone in my organization?Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).Can I connect to any source with an ODBC/JDBC driver?A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction. If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.Can I turn off sample data preview for the entire organization?Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.Can site renaming affect the Jira integration?Learn about can site renaming affect the jira integration?.Can the Hive crawler connect to an independent Hive metastore?Learn about can the hive crawler connect to an independent hive metastore?.Can we use a Microsoft SSO login?Learn about can we use a microsoft sso login?.Configure workflow executionLearn about configure workflow execution.Connect data sources for Azure-hosted Atlan instancesThis document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:.Connect on-premises databases to KubernetesYou can configure and use [Atlan's metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access) to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose.Connection issuesResolve common connection and authentication issues when setting up CrateDB connectorConnectorsLearn how to connect your data sources to Atlan. Explore supported connectors, integration patterns, and best practices for unified catalog management.Connectors and capabilitiesLearn about connectors and capabilities.Crawl Aiven KafkaOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.Crawl Amazon AthenaTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl Amazon DynamoDBOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.Crawl Amazon MSKTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl Amazon QuickSightOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.Crawl Amazon RedshiftOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.Crawl Apache KafkaLearn about crawl apache kafka.Crawl AWS GlueOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.Crawl BigIDConfigure and run the Atlan BigID workflow to crawl metadata from BigID.Crawl Confluent KafkaLearn about crawl confluent kafka.Crawl Confluent Schema RegistryOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.Crawl CrateDBConfigure and run the CrateDB crawler to extract metadata from your databaseCrawl Dagster assetsCreate a crawler workflow in Atlan to capture lineage from Dagster assetsCrawl DatabricksTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl DataStax EnterpriseCrawl DataStax EnterpriseCrawl dbtOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.Crawl DomoOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.Crawl FivetranLearn about crawl fivetran.Crawl Google BigQueryOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.Crawl HiveTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl IBM Cognos AnalyticsOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.Crawl Informatica CDI assetsConfigure and run the crawler to discover and catalog your Informatica CDI assetsCrawl LookerOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.Crawl MatillionOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.Crawl MetabaseOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.Crawl Microsoft Azure Cosmos DBOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.Crawl Microsoft Azure Data FactoryOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.Crawl Microsoft Azure Event HubsOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.Crawl Microsoft Azure Synapse AnalyticsOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.Crawl Microsoft Power BIOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.Crawl Microsoft SQL ServerOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.Crawl MicroStrategyOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.Crawl ModeOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.Crawl MongoDBOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.Crawl Monte CarloOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.Crawl MySQLTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl on-premises databasesOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.Crawl on-premises DatabricksOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.Crawl on-premises IBM Cognos AnalyticsOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.Crawl on-premises KafkaOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.Crawl on-premises LookerOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.Crawl on-premises TableauOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.Crawl on-premises ThoughtSpotOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.Crawl OracleOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.Crawl PostgreSQLTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl PrestoSQLOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.Crawl Qlik Sense CloudOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.Crawl Qlik Sense Enterprise on WindowsOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.Crawl RedashOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.Crawl Redpanda KafkaOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.Crawl SalesforceOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.Crawl SAP HANAOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.Crawl SigmaOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.Crawl SisenseOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.Crawl SnowflakeTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl SodaOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.Crawl TableauTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl TeradataOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.Crawl ThoughtSpotOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.Crawl TrinoTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Create README templatesAdmin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and [enrich their assets with READMEs](/product/integrations). They will also be able to see a rich preview of each template before adding the relevant documentation.Custom solutionsLearn about custom solutions.Dagster integrationFrequently asked questions about Dagster integration with AtlanData Connections and IntegrationComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.Data PipelinesLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.Delete a connectionLearn about delete a connection.Deployment architectureThe Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations.Does Atlan require an admin user in Salesforce?No. However, it is recommended that a Salesforce administrator establishes a [connection between Atlan and Salesforce](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce). To learn more, see [here](/apps/connectors/crm/salesforce/troubleshooting/troubleshooting-salesforce-connectivity).Does lineage only cover calculated fields for Tableau dashboards?Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.Download impacted assets in Google SheetsOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).Enable Azure AD for SSOSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).Enable Google for SSOSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).Enable JumpCloud for SSOSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).Enable Okta for SSOSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).Enable OneLogin for SSOSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).Enable SAML 2.0 for SSOSSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over [here](/product/integrations/identity-management/sso/troubleshooting/troubleshooting-sso).Enable Snowflake OAuthAtlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware.Enable SSO for Amazon RedshiftYou will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift).Enable SSO for Google BigQueryCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.Enable Okta for SCIM provisioningYou can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM).Enrich Atlan through dbtBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.ETL tools connectorsOverview and entry point for all ETL tools connectors in Atlan.extract lineage and usage from DatabricksOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.extract on-premises Databricks lineageOnce you have [set up the databricks-extractor tool](/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction), you can extract lineage from your on-premises Databricks instances by completing the following steps.Find assets by usageData teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance.How are product updates deployed?Learn about how are product updates deployed?.How can I identify an Insights query in my database access log?Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs.How does Atlan handle lineage from Spark jobs?Learn about how does atlan handle lineage from spark jobs?.Implement OpenLineage in Airflow operatorsIf you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage.Infrastructure securityLearn about infrastructure security.Integrate Amazon MWAA/OpenLineageTo learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).Integrate AnomaloOnce you have [configured the Anomalo settings](/apps/connectors/observability/anomalo/how-tos/set-up-anomalo), you can establish a connection between Atlan and Anomalo.Integrate Apache Airflow/OpenLineageTo integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).Integrate Astronomer/OpenLineageTo integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/.Integrate Atlan with Google SheetsThe Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan.Integrate Jira CloudYou must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work.Integrate Microsoft TeamsOnce you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams.Interpret usage metricsAtlan currently supports usage and popularity metrics for the following connectors:Is there a way to build lineage from NetSuite to Snowflake?Learn about is there a way to build lineage from netsuite to snowflake?.Link your Jira accountTo create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that [set up the Jira integration](/product/integrations/project-management/jira/how-tos/integrate-jira-cloud), but not for other users.Link your Microsoft Teams accountTo get alerts for [starred assets](/product/capabilities/discovery/how-tos/star-assets) directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that [set up the Microsoft Teams integration](/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams), but not for other users.Link your Slack accountTo see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that [set up the Slack integration](/product/integrations/collaboration/slack/how-tos/integrate-slack), but not for other users.Manage connectivityOnce you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent.Manage Databricks tagsYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.Manage dbt tagsAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.Manage Google BigQuery tagsAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).Manage requestsIf your organization's [Slack account is integrated with Atlan](/product/integrations/collaboration/slack/how-tos/integrate-slack), you will receive Slack notifications when your requests are approved or rejected.Manage Snowflake tagsYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.Migrate from dbt to Atlan actionThe dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/.Mine Amazon RedshiftOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).Mine Google BigQueryOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.Mine Microsoft Azure Synapse AnalyticsLearn about mine microsoft azure synapse analytics.Mine Microsoft Power BIOnce you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics.Mine queries through S3Once you have crawled assets from a supported connector, you can mine query history.Mine SnowflakeOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.Mine TeradataOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.Monitor connectivityMonitor your data connection workflows and identify failures for troubleshooting.Okta first-time login authentication errorLearn about why do i get an authentication error when logging in via okta for the first time?.OpenLineage configuration and facetsLearn about openlineage configuration and facets.Order workflowsThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.Permissions and limitationsFrequently asked questions about CrateDB connector setup, permissions, and limitationsPreflight checks for Aiven KafkaBefore [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne.Preflight checks for Amazon MSKBefore [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti.Preflight checks for Amazon QuickSightThe [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission.Preflight checks for Amazon RedshiftBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.Preflight checks for AnomaloThis check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid.Preflight checks for Apache KafkaBefore [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection.Preflight checks for Confluent Schema RegistryBefore [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca.Preflight checks for CrateDBTechnical validations performed before running the CrateDB crawler to verify connectivity and permissionsPreflight checks for DatabricksBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.Preflight checks for DataStax EnterprisePreflight checks for DataStax EnterprisePreflight checks for dbtThis checks if manifest files are present in the provided bucket and prefix.Preflight checks for DomoAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.Preflight checks for FivetranLearn about preflight checks for fivetran.Preflight checks for Google BigQueryEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).Preflight checks for HiveBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.Preflight checks for LookerFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project#get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management).Preflight checks for MetabaseBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.Preflight checks for Microsoft Azure Data FactoryBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.Preflight checks for Microsoft Azure Synapse AnalyticsThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.Preflight checks for Microsoft Power BIBefore [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run.Preflight checks for Microsoft SQL ServerBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.Preflight checks for MicroStrategyFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html#/Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions.Preflight checks for ModeBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.Preflight checks for Monte CarloBefore [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c.Preflight checks for MySQLBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.Preflight checks for OracleBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.Preflight checks for PostgreSQLBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.Preflight checks for PrestoSQLBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.Preflight checks for Qlik Sense CloudThis check tests for access to datasets and other Qlik objects.Preflight checks for RedashBefore [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti.Preflight checks for Redpanda KafkaBefore [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod.Preflight checks for SalesforceBefore [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co.Preflight checks for SAP S/4HANAPreflight checks for SAP S/4HANA <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />Preflight checks for SigmaFirst, the list of workbooks in the _Include Workbooks_ and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission.Preflight checks for SisenseAtlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.Preflight checks for SnowflakeBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.Preflight checks for TableauThe [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm#server_info) REST API is used to fetch the `restApiVersion` value.Preflight checks for TeradataBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.Preflight checks for TrinoBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.Provide credentials to query dataLearn about provide credentials to query data.Provide credentials to view sample dataLearn about provide credentials to view sample data.Provide SSL certificatesSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).Provider package versions for OpenLineageLearn about provider package versions for openlineage.Report on assetsLearn about report on assets.Report on automationsYou can track asset enrichment through [suggestions from similar assets](/product/integrations/automation/always-on/references/suggestions-from-similar-assets). You can also view top users who have accepted automated suggestions.S3 Inventory Report StructureExpected folder structure and format for S3 inventory reports used by Atlan's S3 crawler for inventory-based ingestion.Set default user roles for SSO:::warning Who can do this? You will need to be an admin user and [configure SSO](/product/integrations/identity-management/sso) with a provider first.Set up a private network link to Amazon Athena:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.Set up a private network link to HiveOnce the Atlan team has confirmed the configuration is ready, please continue with the remaining steps.Set up a private network link to Trino:::warning Who can do this? You will need your AWS administrator involved - you may not have access to run these tasks yourself.Set up Aiven KafkaAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.Set up AlteryxSet up real-time integration between Alteryx and Atlan using OpenLineage to automatically catalog assets and create lineage when workflows run.Set up Amazon DynamoDBLearn about set up amazon dynamodb.Set up Amazon S3Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.Set up AnomaloAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.Set up AWS GlueLearn about set up aws glue.Set up BigIDCreate a BigID system user and API token for Atlan integration.Set up client credentials flowConfigure Salesforce for OAuth 2.0 client credentials authentication in Atlan.Set up Confluent Schema Registry:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.Set up CrateDBConfigure authentication and connection settings for CrateDB connectorSet up DagsterConfigure Dagster integration with Atlan to enable asset and lineage capture from your Dagster assetsSet up dbt Cloud:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.Set up Domo:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.Set up Google BigQueryYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).Set up Google Cloud StorageConfigure Google Cloud Storage for secure metadata ingestion with Atlan.Set up IBM Cognos Analytics:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.Set up Informatica CDIConfigure authentication and user permissions for Informatica Cloud Data Integration connectorSet up Inventory reportsCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.Set up JWT bearer flowConfigure Salesforce for OAuth 2.0 JWT bearer authentication for Atlan.Set up MatillionConfigure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance.Set up Microsoft Azure Event HubsAtlan supports the following authentication methods for Microsoft Azure Event Hubs:.Set up Microsoft Azure Synapse AnalyticsAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.Set up MicroStrategyAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.Set up ModeIf you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email.Set up MongoDBAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.Set up Monte Carlo:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).Set up on-premises Databricks lineage extractionIn some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Microsoft Azure Synapse Analytics miner accessIn some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to [mine query history from the Query Store](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics). For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Teradata miner accessIn some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up PrestoSQLLearn about set up prestosql.Set up Redpanda KafkaAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.Set up SalesforceLearn about setting up Salesforce authentication for Atlan.Set up SisenseAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.Set up Snowflake:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.Set up Teradata:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.Set up ThoughtSpot:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.Set up username-password flowConfigure Salesforce username-password flow for Atlan integration.Supported connections for on-premises databasesThe metadata-extractor tool supports the following connection types.Supported sourcesLearn about supported sources.Task and crawl issuesTroubleshoot Informatica CDI task processing and crawling issues with error, cause, and solution guidance.Tasks, transformations, and lineageLearn about supported tasks, transformations, and lineage generation in the Informatica CDI connectorTransformationsUnderstand how Informatica Cloud Data Integration transformation logic and business rules are discovered and cataloged in AtlanTroubleshooting AWS Glue connectivityLearn about troubleshooting aws glue connectivity.Troubleshooting connector-specific SSO authenticationLearn about troubleshooting connector-specific sso authentication.Troubleshooting Metabase connectivityLearn about troubleshooting metabase connectivity.Troubleshooting Microsoft TeamsWhy do I get an error while adding Atlan to Microsoft Teams?Troubleshooting Mode connectivityLearn about troubleshooting mode connectivity.Troubleshooting Redash connectivityLearn about troubleshooting redash connectivity.Troubleshooting SCIM provisioningLearn about troubleshooting scim provisioning.Troubleshooting ServiceNowWhy is the security\_admin role required to complete the ServiceNow integration?Troubleshooting Sisense connectivityLearn about troubleshooting sisense connectivity.Troubleshooting SlackWhat do the colors in Slack notifications for modified assets mean?Troubleshooting spreadsheetsWhy do I need admin consent for exporting assets to Microsoft Excel?Troubleshooting ThoughtSpot connectivityLearn about troubleshooting thoughtspot connectivity.update column metadata in Google SheetsOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.Update column metadata in Microsoft ExcelOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.View event logsEvent logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days.What are user roles?Learn about what are user roles?.What does Atlan crawl from Aiven Kafka?Atlan crawls and maps the following assets and properties from Aiven Kafka.What does Atlan crawl from Amazon MSK?Atlan crawls and maps the following assets and properties from Amazon MSK.What does Atlan crawl from Amazon MWAA/OpenLineage?Once you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [.What does Atlan crawl from Amazon QuickSight?Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.What does Atlan crawl from Anomalo?Once you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.What does Atlan crawl from Apache Airflow/OpenLineage?Once you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),.What does Atlan crawl from Apache Kafka?Atlan crawls and maps the following assets and properties from Apache Kafka.What does Atlan crawl from Apache Spark/OpenLineage?Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.What does Atlan crawl from Astronomer/OpenLineage?Atlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).What does Atlan crawl from BigID?Reference guide for BigID metadata crawled by Atlan.What does Atlan crawl from Cloudera Impala?Learn about what does atlan crawl from cloudera impala?.What does Atlan crawl from Confluent Kafka?Atlan crawls and maps the following assets and properties from Confluent Kafka.What does Atlan crawl from Confluent Schema Registry?Atlan crawls and maps the following assets and properties from Confluent Schema Registry.What does Atlan crawl from CrateDB?Complete list of CrateDB assets and metadata properties extracted by Atlan during crawlingWhat does Atlan crawl from DagsterLearn about the Dagster metadata that Atlan captures and visualizesWhat does Atlan crawl from Fivetran?Learn about what does atlan crawl from fivetran?.What does Atlan crawl from Google BigQuery?Atlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data) that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).What does Atlan crawl from Google Cloud Composer/OpenLineage?Atlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).What does Atlan crawl from IBM Cognos Analytics?Atlan crawls and maps the following assets and properties from IBM Cognos Analytics.What does Atlan crawl from Informatica CDIUnderstand the metadata and assets discovered during crawling from Informatica Cloud Data IntegrationWhat does Atlan crawl from Matillion?Atlan crawls and maps the following assets and properties from Matillion.What does Atlan crawl from Metabase?Atlan crawls and maps the following assets and properties from Metabase.What does Atlan crawl from Microsoft Azure Cosmos DB?Once you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.What does Atlan crawl from Microsoft Azure Event Hubs?Atlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs.What does Atlan crawl from Microsoft Power BI?Atlan crawls and maps the following assets and properties from Microsoft Power BI.What does Atlan crawl from MicroStrategy?Atlan crawls and maps the following assets and properties from MicroStrategy.What does Atlan crawl from Mode?Atlan crawls and maps the following assets and properties from Mode.What does Atlan crawl from MongoDB?Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.What does Atlan crawl from Monte Carlo?What does Atlan crawl from Monte Carlo? <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />What does Atlan crawl from MySQL?Atlan crawls and maps the following assets and properties from MySQL.What does Atlan crawl from PostgreSQL?Atlan crawls and maps the following assets and properties from PostgreSQL.What does Atlan crawl from Qlik Sense Cloud?Atlan crawls and maps the following assets and properties from Qlik Sense Cloud.What does Atlan crawl from Qlik Sense Enterprise on Windows?Atlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows.What does Atlan crawl from Redash?Atlan crawls and maps the following assets and properties from Redash.What does Atlan crawl from Redpanda Kafka?Atlan crawls and maps the following assets and properties from Redpanda Kafka.What does Atlan crawl from Sisense?Atlan crawls and maps the following assets and properties from Sisense.What does Atlan crawl from Snowflake?Atlan crawls and maps the following assets and properties from Snowflake.What does Atlan crawl from Soda?Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.What does Atlan crawl from Tableau?Atlan crawls and maps the following assets and properties from Tableau.What does Atlan crawl from ThoughtSpot?Once you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters].What is the crawler logic for a deprecated asset?Learn about what is the crawler logic for a deprecated asset?.What lineage does Atlan extract from Matillion?Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.What lineage does Atlan extract from Microsoft Azure Data Factory?Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).What lineage does Atlan extract from Microsoft Azure Synapse Analytics?Learn about what lineage does atlan extract from microsoft azure synapse analytics?.What lineage does Atlan extract from Microsoft Power BI?This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.What type of user provisioning does Atlan support for SSO integrations?Atlan currently supports _System for Cross-domain Identity Management_ (SCIM) capabilities for user provisioning for:.What's the difference between connecting to Athena and Glue?Learn about what's the difference between connecting to athena and glue?.Why did my users not receive an invite email from Atlan?If you have sent an invite from Atlan but your user(s) did not receive it, Atlan recommends the following:.Why do I get an error message when I click on Atlan's browser extension?Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).Why does the description from Salesforce not show up in Atlan?Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce).Why is Atlan's browser extension not loading?Refer to [Troubleshooting the Atlan browser extension](/product/integrations/automation/browser-extension/troubleshooting/troubleshooting-atlan-browser-extension).WorkflowMonitor workflow runs, view asset creation, track lineage coverage, and access connection metadata.
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/tags/data
Title: 255 docs tagged with "data" | Atlan Documentation
Content: 255 docs tagged with "data"View all tagsAccess archived assetsLearn about access archived assets.Add contract impact analysis in GitHubAdd contract impact analysis in GitHub <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />Add custom metadata<div style={{position: "relative", paddingBottom: "calc(66.33333333333333% + 41px)", height: 0}}> <iframe src="https://demo.arcade.software/1dT1bPneM5fp1O71lb.Add descriptionsYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.Add impact analysis in GitLabLearn about add impact analysis in gitlab.Add options:::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties.AI and Automation FeaturesGuide to Atlan's AI capabilities and automation features for enhanced data governance and productivity.Atlan AI securityAtlan uses [Azure OpenAI Service](https://azure.microsoft.com/en-in/products/cognitive-services/openai-service) to power Atlan AI. Specifically, Atlan uses GPT-4o, a large, pretrained AI model.Attach a tagAtlan allows users to add [tags](/product/capabilities/governance/tags/concepts/what-are-tags) to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection.Automate data profiling‚ûïAvailable via the Data Quality Studio packageCan Atlan read a dump of SQL statements to create lineage?Atlan supports column-level lineage generation for cloud data warehouses like Snowflake, Amazon Redshift, Google BigQuery, and more. Atlan [mines SQL queries](/.Can I be notified if there is a change in downstream dashboards or a schema drift?You can [create webhooks](/product/integrations/automation/webhooks/how-tos/create-webhooks) in Atlan to configure alerts or triggers for downstream actions for metadata change events, including schema changes. You can also configure alerts for asset creation or deletion events.Can I connect to any source with an ODBC/JDBC driver?A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction. If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.Can I query any DW/DL?You can query any data warehouse (DW) or data lake (DL) if the integration is supported via Atlan's [supported sources](/product/connections/references/supported-sources#data-sources). Once integrated, you will be able to query the underlying data using the [Insights](/product/capabilities/insights/how-tos/query-data) feature.Can I turn off sample data preview for the entire organization?Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.Configure Snowflake data metric functionsConfigure Snowflake data metric functions <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />Connect data sources for Azure-hosted Atlan instancesThis document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following:.Connect on-premises databases to KubernetesYou can configure and use [Atlan's metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access) to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose.Crawl Aiven KafkaOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.Crawl Amazon AthenaTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl Amazon DynamoDBOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.Crawl Amazon MSKTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl Amazon QuickSightOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.Crawl Amazon RedshiftOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.Crawl Apache KafkaLearn about crawl apache kafka.Crawl AWS GlueOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.Crawl BigIDConfigure and run the Atlan BigID workflow to crawl metadata from BigID.Crawl Confluent KafkaLearn about crawl confluent kafka.Crawl Confluent Schema RegistryOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.Crawl CrateDBConfigure and run the CrateDB crawler to extract metadata from your databaseCrawl DatabricksTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl DataStax EnterpriseCrawl DataStax EnterpriseCrawl dbtOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.Crawl DomoOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.Crawl FivetranLearn about crawl fivetran.Crawl Google BigQueryOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.Crawl HiveTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl IBM Cognos AnalyticsOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.Crawl LookerOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.Crawl MatillionOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.Crawl MetabaseOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.Crawl Microsoft Azure Cosmos DBOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.Crawl Microsoft Azure Data FactoryOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.Crawl Microsoft Azure Event HubsOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.Crawl Microsoft Azure Synapse AnalyticsOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.Crawl Microsoft Power BIOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.Crawl Microsoft SQL ServerOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.Crawl MicroStrategyOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.Crawl ModeOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.Crawl MongoDBOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.Crawl Monte CarloOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.Crawl MySQLTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl on-premises databasesOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.Crawl on-premises DatabricksOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.Crawl on-premises IBM Cognos AnalyticsOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.Crawl on-premises KafkaOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.Crawl on-premises LookerOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.Crawl on-premises TableauOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.Crawl on-premises ThoughtSpotOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.Crawl OracleOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.Crawl PostgreSQLTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl PrestoSQLOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.Crawl Qlik Sense CloudOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.Crawl Qlik Sense Enterprise on WindowsOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.Crawl RedashOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.Crawl Redpanda KafkaOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.Crawl SalesforceOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.Crawl SAP HANAOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.Crawl SigmaOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.Crawl SisenseOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.Crawl SnowflakeTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl SodaOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.Crawl TableauTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl TeradataOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.Crawl ThoughtSpotOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.Crawl TrinoTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Create announcementsAdding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions.Data Connections and IntegrationComplete guide for connecting Atlan to your data sources, managing integrations, and troubleshooting connection issues.Data ModelsData models provide a framework to describe how data is structured, organized, and related within a system. It acts as a blueprint for organizations to design their business applications and processes. Data models can be of different types: relational, hierarchical, entity relationship, and network.Data PipelinesLearn how to connect your data pipelines to Atlan. Explore ETL tools, workflow orchestration, and lineage tracking to build a comprehensive view of your data movement.Disable data access:::warning Who can do this? You will need to be an admin user in Atlan to configure these options.Discovery FAQsFrequently asked questions about Atlan's Discovery capabilities.Does lineage only cover calculated fields for Tableau dashboards?Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.Download impacted assets in Microsoft ExcelOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for [impact analysis](/product/capabilities/lineage/concepts/what-is-lineage#impact-analysis).Enable Snowflake OAuthAtlan supports [Snowflake OAuth-based authentication](https://docs.snowflake.com/user-guide/oauth-snowflake-overview) for [Snowflake](/apps/connectors/data-ware.Enable SSO for Amazon RedshiftYou will need to [create a client application in Okta](https://help.okta.com/en-us/Content/Topics/Apps/Apps_App_Integration_Wizard_OIDC.htm) to use for [configuring the identity provider in AWS](/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift).Enable SSO for Google BigQueryCredentials are used to obtain an access token from Google's authorization servers for authentication in Atlan.Enrich Atlan through dbtBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.extract lineage and usage from DatabricksOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.extract on-premises Databricks lineageOnce you have [set up the databricks-extractor tool](/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction), you can extract lineage from your on-premises Databricks instances by completing the following steps.Find assets by usageData teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance.How can I identify an Insights query in my database access log?Atlan appends the product name Atlan and a unique ID at the end of each query in a comment. This can help you identify queries from Insights in your database access logs.How can I use personas to update a term in a glossary?By default, any user in Atlan can view all [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) and nested categories and terms in the _Glossary_ section.Implement OpenLineage in Airflow operatorsIf you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage.Integrate Amazon MWAA/OpenLineageTo learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).Integrate Apache Airflow/OpenLineageTo integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).Integrate Apache Spark/OpenLineageAtlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to [OpenLineage configuration and facets](/product/connections/references/openlineage-configuration-and-facets).Integrate Atlan with Microsoft ExcelThe Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel.Integrate Jira Data CenterYou will need to [configure an incoming link](https://confluence.atlassian.com/adminjiraserver/configure-an-incoming-link-1115659067.html) with an external application - in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider.Integrate ServiceNowIf your Atlan admin has [enabled the governance workflows and inbox module](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) in your Atlan workspace, you can create a ServiceNow integration to allow your users to [grant or revoke data access](/product/capabilities/governance/stewardship/how-tos/automate-data-governance) for governed assets in Atlan or any other data source.Interpret usage metricsAtlan currently supports usage and popularity metrics for the following connectors:Link your accountTo [export assets to and bulk enrich metadata from](/product/integrations/collaboration/spreadsheets/how-tos/export-assets) a supported spreadsheet tool,.Link your ServiceNow accountTo request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that [set up the ServiceNow integration](/product/integrations/project-management/servicenow/how-tos/integrate-servicenow), but not for other users.Manage custom metadata structures:::warning Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones.Manage Databricks tagsYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.Manage Google BigQuery tagsAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).Manage Snowflake tagsYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.Migrate from dbt to Atlan actionThe dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a [GitHub](/apps/connectors/etl-tools/dbt/how-tos/.Mine Amazon RedshiftOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).Mine Google BigQueryOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.Mine Microsoft Azure Synapse AnalyticsLearn about mine microsoft azure synapse analytics.Mine queries through S3Once you have crawled assets from a supported connector, you can mine query history.Mine SnowflakeOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.Mine TeradataOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.Order workflowsThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.Preflight checks for Amazon RedshiftBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.Preflight checks for AnomaloThis check tests for the validity of the [host name URL and API key](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo) you provided. If Atlan is unable to connect to your Anomalo instance, this may indicate that your credentials are either incorrect or invalid.Preflight checks for DatabricksBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.Preflight checks for DataStax EnterprisePreflight checks for DataStax EnterprisePreflight checks for DomoAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.Preflight checks for FivetranLearn about preflight checks for fivetran.Preflight checks for Google BigQueryEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).Preflight checks for HiveBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.Preflight checks for MetabaseBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.Preflight checks for Microsoft Azure Data FactoryBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.Preflight checks for Microsoft Azure Synapse AnalyticsThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.Preflight checks for Microsoft SQL ServerBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.Preflight checks for ModeBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.Preflight checks for MySQLBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.Preflight checks for OracleBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.Preflight checks for PostgreSQLBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.Preflight checks for PrestoSQLBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.Preflight checks for Qlik Sense CloudThis check tests for access to datasets and other Qlik objects.Preflight checks for SAP S/4HANAPreflight checks for SAP S/4HANA <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />Preflight checks for SnowflakeBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.Preflight checks for SodaLearn about preflight checks for sodaPreflight checks for TeradataBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.Preflight checks for TrinoBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.Provide SSL certificatesSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).Schedule a queryYou must [save your query](/product/capabilities/insights/how-tos/save-and-share-queries) before you can schedule it. Your [SMTP configuration](/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp) must also be in a working state to send results to recipients.Search and discover assetsAtlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context.SecurityThe Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems. This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring.Security and ComplianceComplete guide to Atlan's security features, compliance certifications, and data protection capabilities.Set up a private network link to Amazon Athena:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.Set up Aiven KafkaAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.Set up Amazon DynamoDBLearn about set up amazon dynamodb.Set up Amazon MSKLearn about set up amazon msk.Set up Amazon QuickSightLearn about set up amazon quicksight.Set up Amazon Redshift:::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself.Set up Amazon S3Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.Set up an Azure private network link to DatabricksFor all details, see [Databricks documentation](https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/private-link-simplified?source=recommendations#create-the-workspace-and-private-endpoints-in-the-azure-portal-ui).Set up AnomaloAtlan supports the API authentication method for fetching metadata from [Anomalo](https://docs.anomalo.com/integrations/atlan-integration). This method uses an API key to fetch metadata.Set up AWS GlueLearn about set up aws glue.Set up BigIDCreate a BigID system user and API token for Atlan integration.Set up Confluent KafkaAtlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata.Set up Confluent Schema Registry:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.Set up DatabricksAtlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods:.Set up DataStax EnterpriseSet up DataStax EnterpriseSet up dbt Cloud:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.Set up Domo:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.Set up FivetranLearn about set up fivetran.Set up Google BigQueryYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).Set up Google Cloud StorageConfigure Google Cloud Storage for secure metadata ingestion with Atlan.Set up Hive:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.Set up IBM Cognos Analytics:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.Set up Inventory reportsCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.Set up Microsoft Azure Cosmos DBIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db).Set up Microsoft Azure Data FactoryAtlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata.Set up Microsoft Azure Event HubsAtlan supports the following authentication methods for Microsoft Azure Event Hubs:.Set up Microsoft Azure Synapse AnalyticsAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.Set up Microsoft Power BIThis guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking.Set up Microsoft SQL Server:::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself.Set up MicroStrategyAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.Set up MongoDBAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.Set up Monte Carlo:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).Set up MySQL:::warning Who can do this? You will probably need your MySQL administrator to run these commands - you may not have access yourself.Set up on-premises database accessIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.Set up on-premises Databricks accessIn some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises IBM Cognos Analytics access:::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,.Set up on-premises Kafka accessIn some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Looker accessIn some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Microsoft Azure Synapse Analytics miner accessIn some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to [mine query history from the Query Store](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics). For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Tableau accessIn some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Teradata miner accessIn some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises ThoughtSpot accessIn some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up Oracle:::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself.Set up PostgreSQL:::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself.Set up PrestoSQLLearn about set up prestosql.Set up Redash:::warning Who can do this? You will probably need your Redash administrator to complete the following steps - you may not have access yourself.Set up Redpanda KafkaAtlan supports the [S3 extraction method](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access) for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata.Set up SAP HANA:::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself.Set up SisenseAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.Set up Snowflake:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.Set up Soda:::warning Who can do this? You will need your [Soda Cloud administrator](https://docs.soda.io/soda-cloud/roles-and-rights.html) to complete these steps -.Set up Tableau:::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself.Set up Teradata:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.SSO integration with PingFederate using SAMLTo use both IdP- and SP-initiated SSO, add both the URLs mentioned above.Star assets:::warning Who can do this? Anyone with access to Atlan - admin, member, or guest user - can star assets.Supported connections for on-premises databasesThe metadata-extractor tool supports the following connection types.Tags and Metadata ManagementComplete guide to managing tags, classifications, and metadata in Atlan for effective data governance and organization.Troubleshooting data modelsWhat are the known limitations of data models in Atlan?Troubleshooting JiraWhat fields are supported when creating tickets or requesting access?Troubleshooting lineageSo you've crawled your source, and mined the queries, but lineage is missing. Why?update column metadata in Google SheetsOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.Update column metadata in Microsoft ExcelOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.Use the filters You can refine the search for your assets in Atlan using the filters . Add filters to your asset search to find assets that are more relevant to you.view data modelsOnce you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:.View query logsYou can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization.What are Power BI processes on the lineage graph?Note that process entities may not have a counterpart entity in Microsoft Power BI. Consider these to be nodes that you can enrich with metadata to describe the process or relationship between two Microsoft Power BI assets.What does Atlan crawl from Amazon Athena?Atlan crawls and maps the following assets and properties from Amazon Athena.What does Atlan crawl from Amazon DynamoDB?Atlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran.What does Atlan crawl from Amazon QuickSight?Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.What does Atlan crawl from Amazon Redshift?Atlan crawls and maps the following assets and properties from Amazon Redshift.What does Atlan crawl from Anomalo?Once you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.What does Atlan crawl from Apache Spark/OpenLineage?Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.What does Atlan crawl from AWS Glue?Atlan crawls and maps the following assets and properties from AWS Glue.What does Atlan crawl from BigID?Reference guide for BigID metadata crawled by Atlan.What does Atlan crawl from Databricks?Atlan crawls and maps the following assets and properties from Databricks.What does Atlan crawl from DataStax Enterprise?What does Atlan crawl from DataStax Enterprise?What does Atlan crawl from Domo?Atlan supports lineage for the following asset types:.What does Atlan crawl from Fivetran?Learn about what does atlan crawl from fivetran?.What does Atlan crawl from Google BigQuery?Atlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data) that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).What does Atlan crawl from Hive?Atlan crawls and maps the following assets and properties from Hive.What does Atlan crawl from Microsoft Azure Cosmos DB?Once you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.What does Atlan crawl from Microsoft Azure Data Factory?Atlan crawls and maps the following assets and properties from Microsoft Azure Data Factory.What does Atlan crawl from Microsoft Azure Synapse Analytics?Atlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources.What does Atlan crawl from Microsoft SQL Server?Atlan crawls and maps the following assets and properties from Microsoft SQL Server.What does Atlan crawl from MongoDB?Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.What does Atlan crawl from MySQL?Atlan crawls and maps the following assets and properties from MySQL.What does Atlan crawl from Oracle?Atlan crawls and maps the following assets and properties from Oracle.What does Atlan crawl from PostgreSQL?Atlan crawls and maps the following assets and properties from PostgreSQL.What does Atlan crawl from PrestoSQL?Atlan crawls and maps the following assets and properties from PrestoSQL.What does Atlan crawl from Qlik Sense Cloud?Atlan crawls and maps the following assets and properties from Qlik Sense Cloud.What does Atlan crawl from SAP ECC?What does Atlan crawl from SAP ECC? <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />What does Atlan crawl from SAP S/4HANA?What does Atlan crawl from SAP S/4HANA? <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />What does Atlan crawl from Sisense?Atlan crawls and maps the following assets and properties from Sisense.What does Atlan crawl from Snowflake?Atlan crawls and maps the following assets and properties from Snowflake.What does Atlan crawl from Soda?Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.What does Atlan crawl from Tableau?Atlan crawls and maps the following assets and properties from Tableau.What does Atlan crawl from Teradata?Atlan crawls and maps the following assets and properties from Teradata.What does Atlan crawl from Trino?Atlan crawls and maps the following assets and properties from Trino.What is included in the Jira integration?With two of your most important workspaces connected, you can save time and improve the way you track issues for your data.What is included in the Microsoft Teams integration?With two of your most important workspaces connected, you can save time and improve the way you share data assets with your team.What is included in the Slack integration?Learn about the features and capabilities of the Slack integration with Atlan.What is the default permission for a glossary?By default, users can search and discover [glossaries](/product/capabilities/governance/glossary/concepts/what-is-a-glossary) in Atlan, irrespective of their user role. The rationale being that glossaries are meant to be accessible to all users who want to understand business context. You can define a [glossary policy](/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data#glossary-policies) to control what users can do with glossary metadata and [create a persona](/product/capabilities/governance/access-control/how-tos/create-a-persona) to curate edit access.What is the difference between a Power BI data source and dataflow?Learn about what is the difference between a power bi data source and dataflow?.What lineage does Atlan extract from Matillion?Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.What lineage does Atlan extract from Microsoft Azure Data Factory?Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).What lineage does Atlan extract from Microsoft Azure Synapse Analytics?Learn about what lineage does atlan extract from microsoft azure synapse analytics?.What lineage does Atlan extract from Microsoft Power BI?This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.When does Atlan become a personal data processor or subprocessor?Atlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.Why do I only see tables from the same schema to join from in a visual query?When [creating a visual query](/product/capabilities/insights/how-tos/query-data), Atlan recommends that you do not select a database or schema in the editor context. Leaving both blank will allow you to discover more tables to join in the Visual Query Builder.Why does the description from Salesforce not show up in Atlan?Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce).Why is lineage available for table level but not column level?The home icon on top of any asset on the [lineage graph](/product/capabilities/lineage/how-tos/view-lineage) indicates the current asset in focus. The lineage view will be different based on the asset you're viewing. To view column-level lineage for [supported sources](/product/connections/references/supported-sources), click **view columns** and then select a column to view data flows for that particular asset.Workflows and Data ProcessingEverything about managing data workflows, understanding lineage generation, and optimizing data processing pipelines in Atlan.
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/tags/crawl
Title: 227 docs tagged with "crawl" | Atlan Documentation
Content: 227 docs tagged with "crawl"View all tagsAdd descriptionsYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.Add options:::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties.Automate data profiling‚ûïAvailable via the Data Quality Studio packageCan I connect to any source with an ODBC/JDBC driver?A number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction. If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.Can I turn off sample data preview for the entire organization?Atlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.Crawl Aiven KafkaOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.Crawl Amazon AthenaTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl Amazon DynamoDBOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.Crawl Amazon MSKTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl Amazon QuickSightOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.Crawl Amazon RedshiftOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.Crawl Apache KafkaLearn about crawl apache kafka.Crawl AWS GlueOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.Crawl BigIDConfigure and run the Atlan BigID workflow to crawl metadata from BigID.Crawl Confluent KafkaLearn about crawl confluent kafka.Crawl Confluent Schema RegistryOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.Crawl CrateDBConfigure and run the CrateDB crawler to extract metadata from your databaseCrawl DatabricksTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl DataStax EnterpriseCrawl DataStax EnterpriseCrawl dbtOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.Crawl DomoOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.Crawl FivetranLearn about crawl fivetran.Crawl GCS assetsConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.Crawl Google BigQueryOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.Crawl HiveTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl IBM Cognos AnalyticsOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.Crawl Informatica CDI assetsConfigure and run the crawler to discover and catalog your Informatica CDI assetsCrawl LookerOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.Crawl MatillionOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.Crawl MetabaseOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.Crawl Microsoft Azure Cosmos DBOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.Crawl Microsoft Azure Data FactoryOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.Crawl Microsoft Azure Event HubsOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.Crawl Microsoft Azure Synapse AnalyticsOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.Crawl Microsoft Power BIOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.Crawl Microsoft SQL ServerOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.Crawl MicroStrategyOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.Crawl ModeOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.Crawl MongoDBOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.Crawl Monte CarloOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.Crawl MySQLTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl on-premises databasesOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.Crawl on-premises DatabricksOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.Crawl on-premises IBM Cognos AnalyticsOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.Crawl on-premises KafkaOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.Crawl on-premises LookerOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.Crawl on-premises TableauOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.Crawl on-premises ThoughtSpotOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.Crawl OracleOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.Crawl PostgreSQLTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl PrestoSQLOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.Crawl Qlik Sense CloudOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.Crawl Qlik Sense Enterprise on WindowsOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.Crawl RedashOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.Crawl Redpanda KafkaOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.Crawl S3 assetsConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.Crawl SalesforceOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.Crawl SAP ECCTo crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl SAP HANAOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.Crawl SAP S/4HANATo crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl SigmaOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.Crawl SisenseOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.Crawl SnowflakeTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl SodaOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.Crawl TableauTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Crawl TeradataOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.Crawl ThoughtSpotOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.Crawl TrinoTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.Disable data access:::warning Who can do this? You will need to be an admin user in Atlan to configure these options.Does lineage only cover calculated fields for Tableau dashboards?Atlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.Enrich Atlan through dbtBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.extract lineage and usage from DatabricksOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.Manage Databricks tagsYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.Manage dbt tagsAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.Manage Google BigQuery tagsAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).Manage Snowflake tagsYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.Mine Amazon RedshiftOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).Mine Google BigQueryOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.Mine Microsoft Azure Synapse AnalyticsLearn about mine microsoft azure synapse analytics.Mine Microsoft Power BIOnce you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics.Mine queries through S3Once you have crawled assets from a supported connector, you can mine query history.Mine SnowflakeOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.Mine TeradataOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.Order workflowsThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.Preflight checks for Aiven KafkaBefore [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne.Preflight checks for Amazon MSKBefore [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti.Preflight checks for Amazon QuickSightThe [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission.Preflight checks for Amazon RedshiftBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.Preflight checks for Apache KafkaBefore [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection.Preflight checks for Confluent Schema RegistryBefore [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca.Preflight checks for DatabricksBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.Preflight checks for DataStax EnterprisePreflight checks for DataStax EnterprisePreflight checks for dbtThis checks if manifest files are present in the provided bucket and prefix.Preflight checks for DomoAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.Preflight checks for FivetranLearn about preflight checks for fivetran.Preflight checks for Google BigQueryEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).Preflight checks for HiveBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.Preflight checks for LookerFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project#get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management).Preflight checks for MetabaseBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.Preflight checks for Microsoft Azure Data FactoryBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.Preflight checks for Microsoft Azure Synapse AnalyticsThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.Preflight checks for Microsoft Power BIBefore [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run.Preflight checks for Microsoft SQL ServerBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.Preflight checks for MicroStrategyFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html#/Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions.Preflight checks for ModeBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.Preflight checks for Monte CarloBefore [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c.Preflight checks for MySQLBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.Preflight checks for OracleBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.Preflight checks for PostgreSQLBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.Preflight checks for PrestoSQLBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.Preflight checks for Qlik Sense CloudThis check tests for access to datasets and other Qlik objects.Preflight checks for RedashBefore [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti.Preflight checks for Redpanda KafkaBefore [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod.Preflight checks for SalesforceBefore [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co.Preflight checks for SAP S/4HANAPreflight checks for SAP S/4HANA <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />Preflight checks for SigmaFirst, the list of workbooks in the _Include Workbooks_ and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission.Preflight checks for SisenseAtlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.Preflight checks for SnowflakeBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.Preflight checks for SodaLearn about preflight checks for sodaPreflight checks for TableauThe [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm#server_info) REST API is used to fetch the `restApiVersion` value.Preflight checks for TeradataBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.Preflight checks for TrinoBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.Provide SSL certificatesSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).Set up a private network link to Amazon Athena:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.Set up Amazon Redshift:::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself.Set up Amazon S3Create AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.Set up AWS GlueLearn about set up aws glue.Set up BigIDCreate a BigID system user and API token for Atlan integration.Set up Confluent Schema Registry:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.Set up DataStax EnterpriseSet up DataStax EnterpriseSet up dbt Cloud:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.Set up Domo:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.Set up FivetranLearn about set up fivetran.Set up Google BigQueryYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).Set up Google Cloud StorageConfigure Google Cloud Storage for secure metadata ingestion with Atlan.Set up Hive:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.Set up IBM Cognos Analytics:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.Set up Inventory reportsCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.Set up Looker:::warning Who can do this? You will probably need your Looker administrator to run these commands - you may not have access yourself.Set up Microsoft Azure Cosmos DBIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db).Set up Microsoft Azure Synapse AnalyticsAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.Set up Microsoft SQL Server:::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself.Set up MicroStrategyAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.Set up ModeIf you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email.Set up MongoDBAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.Set up Monte Carlo:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).Set up on-premises database accessIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.Set up on-premises Databricks accessIn some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises IBM Cognos Analytics access:::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,.Set up on-premises Kafka accessIn some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Looker accessIn some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises Tableau accessIn some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up on-premises ThoughtSpot accessIn some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.Set up Oracle:::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself.Set up PostgreSQL:::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself.Set up SAP HANA:::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself.Set up SisenseAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.Set up Snowflake:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.Set up Tableau:::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself.Set up Teradata:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.Set up ThoughtSpot:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.Set up Trino:::warning Who can do this? You will probably need your Trino administrator to run these commands - you may not have access yourself.Troubleshooting data modelsWhat are the known limitations of data models in Atlan?Troubleshooting lineageSo you've crawled your source, and mined the queries, but lineage is missing. Why?update column metadata in Google SheetsOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.Update column metadata in Microsoft ExcelOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.view data modelsOnce you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:.What does Atlan crawl from Aiven Kafka?Atlan crawls and maps the following assets and properties from Aiven Kafka.What does Atlan crawl from Amazon Athena?Atlan crawls and maps the following assets and properties from Amazon Athena.What does Atlan crawl from Amazon DynamoDB?Atlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran.What does Atlan crawl from Amazon MSK?Atlan crawls and maps the following assets and properties from Amazon MSK.What does Atlan crawl from Amazon MWAA/OpenLineage?Once you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [.What does Atlan crawl from Amazon QuickSight?Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.What does Atlan crawl from Amazon Redshift?Atlan crawls and maps the following assets and properties from Amazon Redshift.What does Atlan crawl from Amazon S3Complete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.What does Atlan crawl from Anomalo?Once you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.What does Atlan crawl from Apache Airflow/OpenLineage?Once you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),.What does Atlan crawl from Apache Kafka?Atlan crawls and maps the following assets and properties from Apache Kafka.What does Atlan crawl from Apache Spark/OpenLineage?Atlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.What does Atlan crawl from Astronomer/OpenLineage?Atlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).What does Atlan crawl from AWS Glue?Atlan crawls and maps the following assets and properties from AWS Glue.What does Atlan crawl from BigID?Reference guide for BigID metadata crawled by Atlan.What does Atlan crawl from Confluent Kafka?Atlan crawls and maps the following assets and properties from Confluent Kafka.What does Atlan crawl from CrateDB?Complete list of CrateDB assets and metadata properties extracted by Atlan during crawlingWhat does Atlan crawl from Databricks?Atlan crawls and maps the following assets and properties from Databricks.What does Atlan crawl from DataStax Enterprise?What does Atlan crawl from DataStax Enterprise?What does Atlan crawl from Domo?Atlan supports lineage for the following asset types:.What does Atlan crawl from Fivetran?Learn about what does atlan crawl from fivetran?.What does Atlan crawl from Google BigQuery?Atlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data) that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).What does Atlan crawl from Google Cloud Composer/OpenLineage?Atlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).What does Atlan crawl from Google GCSComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging.What does Atlan crawl from Hive?Atlan crawls and maps the following assets and properties from Hive.What does Atlan crawl from IBM Cognos Analytics?Atlan crawls and maps the following assets and properties from IBM Cognos Analytics.What does Atlan crawl from Looker?Atlan crawls and maps the following assets and properties from Looker.What does Atlan crawl from Matillion?Atlan crawls and maps the following assets and properties from Matillion.What does Atlan crawl from Microsoft Azure Cosmos DB?Once you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.What does Atlan crawl from Microsoft Azure Data Factory?Atlan crawls and maps the following assets and properties from Microsoft Azure Data Factory.What does Atlan crawl from Microsoft Azure Event Hubs?Atlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs.What does Atlan crawl from Microsoft Azure Synapse Analytics?Atlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources.What does Atlan crawl from Microsoft Power BI?Atlan crawls and maps the following assets and properties from Microsoft Power BI.What does Atlan crawl from Microsoft SQL Server?Atlan crawls and maps the following assets and properties from Microsoft SQL Server.What does Atlan crawl from MicroStrategy?Atlan crawls and maps the following assets and properties from MicroStrategy.What does Atlan crawl from MongoDB?Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.What does Atlan crawl from Monte Carlo?What does Atlan crawl from Monte Carlo? <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />What does Atlan crawl from MySQL?Atlan crawls and maps the following assets and properties from MySQL.What does Atlan crawl from Oracle?Atlan crawls and maps the following assets and properties from Oracle.What does Atlan crawl from PostgreSQL?Atlan crawls and maps the following assets and properties from PostgreSQL.What does Atlan crawl from PrestoSQL?Atlan crawls and maps the following assets and properties from PrestoSQL.What does Atlan crawl from Qlik Sense Cloud?Atlan crawls and maps the following assets and properties from Qlik Sense Cloud.What does Atlan crawl from Qlik Sense Enterprise on Windows?Atlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows.What does Atlan crawl from Redash?Atlan crawls and maps the following assets and properties from Redash.What does Atlan crawl from Redpanda Kafka?Atlan crawls and maps the following assets and properties from Redpanda Kafka.What does Atlan crawl from Salesforce?Atlan only performs GET requests on these five endpoints:.What does Atlan crawl from SAP ECC?What does Atlan crawl from SAP ECC? <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />What does Atlan crawl from SAP S/4HANA?What does Atlan crawl from SAP S/4HANA? <Badge variant="preview" text="Private Preview" link="/get-started/references/product-release-stages#private-preview" />What does Atlan crawl from Sisense?Atlan crawls and maps the following assets and properties from Sisense.What does Atlan crawl from Snowflake?Atlan crawls and maps the following assets and properties from Snowflake.What does Atlan crawl from Soda?Atlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.What does Atlan crawl from Tableau?Atlan crawls and maps the following assets and properties from Tableau.What does Atlan crawl from Teradata?Atlan crawls and maps the following assets and properties from Teradata.What does Atlan crawl from ThoughtSpot?Once you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters].What does Atlan crawl from Trino?Atlan crawls and maps the following assets and properties from Trino.What lineage does Atlan extract from Matillion?Atlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.What lineage does Atlan extract from Microsoft Azure Data Factory?Atlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).What lineage does Atlan extract from Microsoft Power BI?This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.When does Atlan become a personal data processor or subprocessor?Atlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.Why does the description from Salesforce not show up in Atlan?Atlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce).
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-aws-private-network-link-to-snowflake
Title: Set up an AWS private network link to Snowflake | Atlan Documentation
Content: Connect dataData WarehousesSnowflakeGet StartedSet up an AWS private network link to SnowflakeOn this pageSet up an AWS private network link to SnowflakeAWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Snowflake and Atlan, when you use our Single Tenant SaaS deployment. Who can do this?You will need Snowflake Support, and probably your Snowflake administrator involved - you may not have access or the tools to run these tasks. Prerequisites‚Äã Snowflake must be setup with Business Critical Edition (or higher). Open a ticket with Snowflake Support to enable PrivateLink for your Snowflake account. Snowflake support will take 1-2 days to review and enable PrivateLink. If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support request to do so. (For all details, see the Snowflake documentation.) Fetch PrivateLink information‚Äã Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin;select system$get_privatelink_config(); This will produce output like the following (formatted here for readability): { "privatelink-account-name":"abc123.ap-south-1.privatelink", "privatelink-vpce-id":"com.amazonaws.vpce.ap-south-1.vpce-svc-257a4d536bd8e3594", "privatelink-account-url":"abc123.ap-south-1.privatelink.snowflakecomputing.com", "regionless-privatelink-account-url":"xyz789-abc123.privatelink.snowflakecomputing.com", "privatelink_ocsp-url":"ocsp.abc123.ap-south-1.privatelink.snowflakecomputing.com", "privatelink-connection-urls":"[]"} Share details with Atlan support team‚Äã Share the following values with the Atlan support team: privatelink-account-name privatelink-vpce-id privatelink-account-url privatelink_ocsp-url Atlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you. When you use this endpoint in the configuration for crawling and mining, Atlan will connect to Snowflake over the PrivateLink.Tags:atlandocumentationPreviousSet up SnowflakeNextSet up an Azure private network link to SnowflakePrerequisitesFetch PrivateLink informationShare details with Atlan support team
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/databricks
Title: Databricks | Atlan Documentation
Content: Connect dataData WarehousesDatabricksOn this pageDatabricks Overview: Catalog Databricks workspaces, databases, schemas, and tables in Atlan. Gain visibility into lineage, usage, and governance for your Databricks assets. Get started‚Äã Follow these steps to connect and catalog Databricks assets in Atlan: Set up the connector Crawl Databricks assets Guides‚Äã Cross-workspace setup‚Äã Set up cross-workspace extraction: Configure a single service principal to crawl metadata from all workspaces within a Databricks metastore. Lineage & usage‚Äã Extract lineage and usage from Databricks: Extract lineage and usage metrics from your Databricks assets. Tag management‚Äã Manage Databricks tags: Configure and manage tags in Databricks. On-premises‚Äã Set up on-premises Databricks access: Configure Atlan to access on-premises Databricks environments. Set up on-premises Databricks lineage extraction: Prepare for offline lineage extraction from on-premises Databricks. Extract on-premises Databricks lineage: Step-by-step instructions for extracting lineage from on-premises Databricks. Crawl on-premises Databricks: Crawl metadata from on-premises Databricks environments. Private networking‚Äã Set up an AWS private network link to Databricks: Establish a secure, private network connection to Databricks on AWS. Set up an Azure private network link to Databricks: Establish a secure, private network connection to Databricks on Azure. References‚Äã What does Atlan crawl from Databricks: Learn about the Databricks assets and metadata that Atlan discovers and catalogs. Preflight checks for Databricks: Verify prerequisites before setting up the Databricks connector. Troubleshooting‚Äã Databricks connectivity: Resolve common Databricks connection issues and errors. Cross-workspace extraction issues: Troubleshoot common issues in Databricks cross-workspace extraction with error, cause, and solution guidance. FAQ‚Äã Cross-workspace extraction setup: Frequently asked questions about setting up and configuring cross-workspace extraction. Tags:databricksconnectordata warehouseconnectivityNextSet up DatabricksGet startedGuidesReferencesTroubleshootingFAQ
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks
Title: Crawl Databricks | Atlan Documentation
Content: Connect dataData WarehousesDatabricksCrawl Databricks AssetsCrawl DatabricksOn this pageCrawl DatabricksOnce you have configured the Databricks access permissions, you can establish a connection between Atlan and your Databricks instance. (If you are also using AWS PrivateLink or Azure Private Link for Databricks, you will need to set that up first, too.) To crawl metadata from your Databricks instance, review the order of operations and then complete the following steps. Select the source‚Äã To select Databricks as your source: In the top right corner of any screen, navigate to New and then click New Workflow. From the list of packages, select Databricks Assets, and click Setup Workflow. Provide credentials‚Äã Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. Next, select an authentication method: In JDBC, you will need a personal access token and HTTP path for authentication. In AWS Service, you will need a client ID and client secret for AWS service principal authentication. In Azure Service, you will need a tenant ID, client ID, and client secret for Azure service principal authentication. In Offline extraction, you will need to first extract metadata yourself and make it available in S3. In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method‚Äã JDBC‚Äã To enter your Databricks credentials: For Host, enter the hostname, AWS PrivateLink endpoint, or Azure Private Link endpoint for your Databricks instance. For Port, enter the port number of your Databricks instance. For Personal Access Token, enter the access token you generated when setting up access. For HTTP Path, enter one of the following: A path starting with /sql/1.0/warehouses to use the Databricks SQL warehouse. A path starting with sql/protocolv1/o to use the Databricks interactive cluster. Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next. warningMake sure your Databricks instance (SQL warehouse or interactive cluster) is up and running, otherwise the Test Authentication step times out. AWS service principal‚Äã To enter your Databricks credentials: For Host, enter the hostname or AWS PrivateLink endpoint for your Databricks instance. For Port, enter the port number of your Databricks instance. For Client ID, enter the client ID for your AWS service principal. For Client Secret, enter the client secret for your AWS service principal. Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next. Azure service principal‚Äã To enter your Databricks credentials: For Host, enter the hostname or Azure Private Link endpoint for your Databricks instance. For Port, enter the port number of your Databricks instance. For Client ID, enter the application (client) ID for your Azure service principal. For Client Secret, enter the client secret for your Azure service principal. For Tenant ID, enter the directory (tenant) ID for your Azure service principal. Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next. Offline extraction method‚Äã Atlan supports the offline extraction method for fetching metadata from Databricks. This method uses Atlan's databricks-extractor tool to fetch metadata. You need to first extract the metadata yourself and then make it available in S3. To enter your S3 details: For Bucket name, enter the name of your S3 bucket. For Bucket prefix, enter the S3 prefix under which all the metadata files exist. These include output/databricks-example/catalogs/success/result-0.json, output/databricks-example/schemas/{{catalog_name}}/success/result-0.json, output/databricks-example/tables/{{catalog_name}}/success/result-0.json, and similar files. (Optional) For Bucket region, enter the name of the S3 region. When complete, at the bottom of the screen, click Next. Agent extraction method‚Äã Atlan supports using a Secure Agent for fetching metadata from Databricks. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Databricks data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection‚Äã To complete the Databricks connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production, development, gold, or analytics. (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins. warningIf you don't specify any user or group, nobody can manage the connection - not even admins. (Optional) To prevent users from querying any Databricks data, change Enable SQL Query to No. (Optional) To prevent users from previewing any Databricks data, change Enable Data Preview to No. (Optional) To prevent users from running large queries, change Max Row Limit or keep the default selection. At the bottom of the screen, click the Next button to proceed. Configure the crawler‚Äã Before running the Databricks crawler, you can further configure it. System tables extraction method‚Äã The system metadata extraction method is only available for Unity Catalog-enabled workspaces. It provides access to detailed metadata from system tables and supports all three authentication types. You can extract metadata from your Databricks workspace using this method. Follow these steps: Set up authentication using one of the following: Personal access token AWS service principal Azure service principal The default options can work as is. You may choose to override the defaults for any of the remaining options: For Asset selection, select a filtering option: For SQL warehouse, click the dropdown to select the SQL warehouse you want to configure. To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.) To have the crawler include Databases, Schemas, or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases includes all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.) To have the crawler ignore Databases, Schemas, or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views excludes all the matching tables and views. Click + to add more filters. If you add multiple filters, assets are crawled based on matching all the filtering conditions you have set. To import tags from Databricks to Atlan, change Import Tags to Yes. Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Did you know?If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Incremental extraction‚Äã Toggle incremental extraction, for a faster and more efficient metadata extraction. JDBC extraction method‚Äã The JDBC extraction method uses JDBC queries to extract metadata from your Databricks instance. This was the original extraction method provided by Databricks. This extraction method is only supported for personal access token authentication. You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata. (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata. (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For View Definition Lineage, keep the default Yes to generate upstream lineage for views based on the tables referenced in the views or click No to exclude from crawling. For Advanced Config, keep Default for the default configuration or click Advanced to further configure the crawler: To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select True to enable it or False to disable it. REST API extraction method‚Äã The REST API extraction method uses Unity Catalog to extract metadata from your Databricks instance. This extraction method is supported for all three authentication options: personal access token, AWS service principal, and Azure service principal. This method is only supported by Unity Catalog-enabled workspaces. If you enable an existing workspace, you also need to upgrade your tables and views to Unity Catalog. While REST APIs are used to extract metadata, JDBC queries are still used for querying purposes. You can override the defaults for any of these options: Change the extraction method under Extraction method to REST API. To select the assets you want to include in crawling, click Include Metadata. (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata. (This will default to no assets if none are specified.) To import tags from Databricks to Atlan, change Import Tags to Yes. Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. For SQL warehouse, click the dropdown to select the SQL warehouse you have configured. Did you know?If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler‚Äã Follow these steps to run the Databricks crawler: To check for any permissions or other configuration issues before running the crawler, click Preflight checks. You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! üéâTags:connectorsdatacrawlsetupPreviousSet up cross-workspace extractionNextSet up on-premises Databricks accessSelect the sourceProvide credentialsConfigure the connectionConfigure the crawlerRun the crawler
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-cross-workspace-extraction
Title: Set up cross-workspace extraction | Atlan Documentation
Content: Connect dataData WarehousesDatabricksCross-workspace SetupSet up cross-workspace extractionOn this pageSet up cross-workspace extractionEliminate the need for separate crawler configurations by using a single service principal to crawl metadata from all workspaces within a Databricks metastore. This guide walks you through configuring the necessary permissions to enable cross-workspace extraction. Important!Cross-workspace extraction isn't supported for REST API or JDBC extraction methods. Prerequisites‚Äã Before you begin, make sure you have: A Unity Catalog-enabled Databricks workspace Account admin access to create and manage service principals Workspace admin access to grant permissions across all target workspaces At least one active SQL warehouse in each workspace you intend to crawl Set up Databricks authentication completed with one of the supported authentication methods System table extraction enabled for lineage and usage extraction Add service principal to all workspaces‚Äã You must use a single, common service principal that has been granted access to all Databricks workspaces you intend to crawl within the metastore. Log in to your Databricks account console as an account admin From the left , click Workspaces and select a workspace From the tabs along the top, click the Permissions tab In the upper right, click Add permissions In the Add permissions dialog: For User, group, or service principal, select your service principal For Permission, select workspace User Click Add Repeat steps 2-5 for each workspace you intend to crawl Permissions required‚Äã The service principal needs the following permissions on each workspace from which the you want Atlan to extract metadata and to enable cross-workspace extraction: CAN_USE on SQL warehouses in each workspace USE CATALOG on system catalog USE SCHEMA on system.access (for cross-workspace discovery) USE SCHEMA on system.information_schema SELECT on the following system tables: system.access.workspace_latest (for cross-workspace discovery) system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints BROWSE on all catalogs you want to crawl Grant permissions‚Äã Configure the necessary permissions for the service principal to access and extract metadata from all workspaces within the metastore. SQL workspace permissions: The service principal must have usage permissions on at least one active SQL warehouse within each workspace. The extractor uses the smallest available warehouse to run its discovery queries. Via SQLVia UI Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command for each workspace, replacing the placeholders: GRANT CAN_USE ON WAREHOUSE <warehouse_name> TO `<service_principal_id>`; Replace <warehouse_name> with your actual warehouse name Replace <service_principal_id> with your service principal's application ID ExampleGRANT CAN_USE ON WAREHOUSE production-warehouse TO `12345678-1234-1234-1234-123456789012`; Log in to your Databricks workspace as a workspace admin From the left , click SQL Warehouses On the Compute page, for each SQL warehouse, click the 3-dot icon and then click Permissions In the Manage permissions dialog: In the Type to add multiple users or groups field, search for and select your service principal Select Can use permission Click Add to assign the permission System table permissions: Access to the system schema is essential for workspace and lineage discovery. Via SQLVia UI Connect to your Databricks workspace using a SQL client or the SQL editor Grant system catalog access: GRANT USE CATALOG ON CATALOG system TO `<service_principal_id>`; Grant schema-level permissions: GRANT USE SCHEMA ON SCHEMA system.access TO `<service_principal_id>`;GRANT USE SCHEMA ON SCHEMA system.information_schema TO `<service_principal_id>`; Grant SELECT permissions on required system tables: -- For cross-workspace discoveryGRANT SELECT ON TABLE system.access.workspace_latest TO `<service_principal_id>`;-- For metadata extractionGRANT SELECT ON TABLE system.information_schema.catalogs TO `<service_principal_id>`;GRANT SELECT ON TABLE system.information_schema.schemata TO `<service_principal_id>`;GRANT SELECT ON TABLE system.information_schema.tables TO `<service_principal_id>`;GRANT SELECT ON TABLE system.information_schema.columns TO `<service_principal_id>`;GRANT SELECT ON TABLE system.information_schema.key_column_usage TO `<service_principal_id>`;GRANT SELECT ON TABLE system.information_schema.table_constraints TO `<service_principal_id>`; Replace <service_principal_id> with your service principal's application ID ExampleGRANT USE CATALOG ON CATALOG system TO `12345678-1234-1234-1234-123456789012`;GRANT USE SCHEMA ON SCHEMA system.access TO `12345678-1234-1234-1234-123456789012`;GRANT USE SCHEMA ON SCHEMA system.information_schema TO `12345678-1234-1234-1234-123456789012`;GRANT SELECT ON TABLE system.access.workspace_latest TO `12345678-1234-1234-1234-123456789012`;GRANT SELECT ON TABLE system.information_schema.catalogs TO `12345678-1234-1234-1234-123456789012`; Log in to your Databricks workspace as a workspace admin From the left , click Catalog In the Catalog Explorer, click on the system catalog Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals, select your service principal Under Privileges, check USE CATALOG Click Grant to apply the permissions Navigate to system > access Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals, select your service principal Under Privileges, check USE SCHEMA Click Grant Repeat for system > information_schema For each required system table, navigate to the table and grant SELECT permissions: system.access.workspace_latest system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Asset permissions: The service principal requires BROWSE permissions on all catalogs you want to crawl. BROWSE permission enables the service principal to see and read metadata for all data assets within the catalog, automatically granting access to all schemas and tables. Important!For private catalogs, grant permissions from each workspace. For public catalogs, grant from any workspace. Only visible in the system tables when the service principal has BROWSE privileges on individual catalogs. Via SQLVia UI Connect to your Databricks workspace using a SQL client or the SQL editor Grant BROWSE permissions on each catalog you want to crawl: GRANT BROWSE ON CATALOG <catalog_name> TO `<service_principal_id>`; Replace <catalog_name> with your actual catalog name Replace <service_principal_id> with your service principal's application ID ExampleGRANT BROWSE ON CATALOG main TO `12345678-1234-1234-1234-123456789012`; Log in to your Databricks workspace as a workspace admin From the left , click Catalog In the Catalog Explorer, navigate to the catalog you want to grant permissions on (for example, main) Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals, select your service principal Under Privileges, check BROWSE Click Grant to apply the permissions Repeat steps 3-5 for each catalog you want to crawl in Atlan Need help?‚Äã Check Cross-workspace extraction setup FAQ for common questions about cross-workspace extraction Check Troubleshooting cross-workspace extraction issues for common issues Contact Atlan support for help with setup or integration Next steps‚Äã Crawl Databricks - Set up and run a workflow to extract metadata from your Databricks instance using direct, offline, or agent extraction methods Tags:databrickssetupcross-workspace-extractionPreviousSet up DatabricksNextCrawl DatabricksPrerequisitesAdd service principal to all workspacesPermissions requiredGrant permissionsNeed help?Next steps
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/capabilities/insights/how-tos/query-data
Title: query data | Atlan Documentation
Content: Use dataInsightsGet StartedHow to query dataOn this pagequery dataThere are two ways to query data in Atlan: writing your own SQL using the Visual Query Builder Did you know?Atlan pushes all queries to the source (no data is stored in Atlan). In addition, Atlan applies access policies to the results before displaying them. Write your own SQL‚Äã Who can do this?Anyone with the knowledge to write SQL. Any Atlan user with data access to the asset can query data. To query an asset with your own SQL: From the left  of any screen, click Insights. Under the Explorer tab, find the asset you want to query: Use the Select database dropdown to choose another database, if necessary. Search for the asset by name in the search bar, or browse for it in the tree structure. Hover over the table or view, and click the play icon. This writes and runs a basic preview query. (Optional) Click the open asset sidebar icon to view more details in the asset sidebar. (Optional) Click the eye icon to view a preview of the query results. (Optional) Click the 3-dot icon for more options: Click Set editor context to set the same connection, database, and schema name in the query editor as selected in the Explorer tab. Click Place name in editor to view the asset name in the query editor. Click Copy path to copy the full path of the asset, including database and schema names. Under the Untitled tab on the right, change the sample query or write your own - separate multiple queries with a semicolon ;. Click the Run button in the upper right to test your query as you write it. (Optional) Click the downward arrow next to the Run button to export query results via email or schedule the query. (Optional) If you have multiple tabs open in the query editor, right-click a tab to open the tabs . You can close a specific tab or all tabs, or duplicate the query. (Optional) From the top right of the query editor, click the 3-dot icon for additional query editor actions or to customize it further: Click or hover over Duplicate query to create a duplicate version of your query. Click or hover over Open command palette to view the actions you can run inside the query editor. Click or hover over Themes and then select your preferred theme for the query editor. Click or hover over Tab spacing to change the tab spacing for your queries. Click or hover over Font size to change the font size for your queries. Click or hover over Cursor to change the cursor position in the query editor. Click or hover over Autosuggestions to turn off autosuggestions for assets in the query editor. The editor supports all read-based SQL statements, including JOIN. The editor will not run any write-based statements. The following SQL statements are not supported: UPDATE DELETE CREATE ALTER DROP TRUNCATE INSERT INTO Did you know?You can select the context for your query to the left of the Run button. Then you won't need to fully qualify table names with schema and database names. Use the Visual Query Builder‚Äã Who can do this?Any Atlan user with data access to the asset. No SQL knowledge required! To query an asset using the Visual Query Builder: From the left  of any screen, click Insights. At the top of the screen, to the right of the Untitled tab, click the + button and select New visual query. Under Select from choose the table or view you want to query. (Optional) In the column selector to the right, select the column you want to query. Then develop your query: Click the Run button to run the query and preview its results. Click the blue circular + button to add an action to the query. Repeat these steps until your query is complete. (Optional) If there are any errors in your query, click Auto fix for Atlan to recommend a fix. (Optional) In the query results set, click Copy to copy the query results or click Download to export them. Did you know?You can learn more about the query builder actions in this example.Tags:atlandocumentationPreviousInsightsNextSave and share queriesWrite your own SQLUse the Visual Query Builder
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/product/capabilities/discovery/references/provide-credentials-to-view-sample-data
Title: Provide credentials to view sample data | Atlan Documentation
Content: Use dataDiscoveryReferencesProvide credentials to view sample dataOn this pageProvide credentials to view sample dataOnce your connection admins have configured bring your own credentials (BYOC) in Atlan, users will need to provide their own credentials before they can view the sample data in the asset profile. This will help you enforce better governance across your organization. Who can do this?Any Atlan user with data access to the asset and their own credentials for the data store. Atlan will display a 100-row sample of the data. Use your own credentials to view sample data‚Äã Atlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports SSO authentication. To set up your own credentials for viewing sample data: On the Assets page, click on an asset to view its asset profile. In the asset profile, click Sample Data. To set up your credentials for viewing the sample data, click Get Started. In the popup window, click Get Started once again to proceed. In the User credential setup dialog box, Basic is selected as the default authentication option. Enter the following: For Username, enter the username for the connection. For Password, enter the password for that connection. For Role, enter your role for that connection. For Warehouse, enter the name of the warehouse. Click the Test Authentication button to confirm your credentials. Once authentication is successful, click Done. You can now view sample data using your own credentials! üéâ When using the key pair method, you'll need to enter your encrypted private key and the private key password to complete the authentication process. Did you know?Once you've set up your credentials for viewing sample data, you can also manage your credentials. If your admin has enabled sample data download, you can export sample data in a CSV file.Tags:integrationconnectorsPreviousWhat are asset profiles?NextDiscovery FAQsUse your own credentials to view sample data
--------------------------------------------------------------------------------

URL: https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/manage-databricks-tags
Title: Manage Databricks tags | Atlan Documentation
Content: Connect dataData WarehousesDatabricksTag managementManage Databricks tagsOn this pageManage Databricks tagsYou must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Atlan enables you to import your Databricks tags, update your Databricks assets with the imported tags, and push the tag updates back to Databricks: Import tags - crawl Databricks tags from Databricks to Atlan Reverse sync - sync Databricks tag updates from Atlan to Databricks Once you've imported your Databricks tags to Atlan: Your Databricks assets in Atlan will be automatically enriched with their Databricks tags. Imported Databricks tags will be mapped to corresponding Atlan tags through case-insensitive name match - multiple Databricks tags can be matched to a single tag in Atlan. You can also attach Databricks tags, including tag values, to your Databricks assets in Atlan - allowing you to categorize your assets at a more granular level. You can filter your assets by Databricks tags and tag values. You can enable reverse sync to push any tag updates for your Databricks assets back to Databricks - including tag values added to assets in Atlan. Did you know?Enabling reverse sync will only update existing tags in Databricks. It will neither create nor delete any tags in Databricks. Prerequisites‚Äã You must have a Unity Catalog-enabled workspace and SQL warehouse configured to import Databricks tags in Atlan. Before you can import tags from and push tag updates to Databricks using personal access token, AWS service principal, or Azure service principal authentication, you will need to do the following: Ensure that you have a Unity Catalog-enabled workspace and a SQL warehouse configured. Create tags or have existing tags in Databricks. Grant permissions to import tags from and push tag updates to Databricks. Import Databricks tags to Atlan‚Äã Who can do this?You will need to be an admin user in Atlan to import Databricks tags to Atlan. You will also need to work with your Databricks administrator to grant permissions to import tags from Databricks - you may not have access yourself. You can import your Databricks tags to Atlan through one-way tag sync. The synced Databricks tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Databricks assets will be enriched with their synced tags from Databricks. To import Databricks tags to Atlan, you can either: Create a new Databricks workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Databricks workflow to change Import Tags to Yes. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags imported from Databricks will be available to use for tagging assets! üéâ View Databricks tags in Atlan‚Äã Once you've imported your Databricks tags, you will be able to view and manage your Databricks tags in Atlan. To view Databricks tags: From the left  of any screen, click Governance. Under the Governance heading of the _Governance cente_r, click Tags. (Optional) Under Tags, click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. From the left  under Tags, select a synced tag. In the Overview section, you can view a total count of synced Databricks tags. To the right of Overview, click Synced tags to view additional details - including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Databricks tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon. You cannot rename tags synced from Databricks. Push tag updates to Databricks‚Äã Who can do this?Any admin or member user in Atlan can configure reverse sync for tag updates to Databricks. You will also need to work with your Databricks administrator to grant additional permissions to push updates - you may not have access yourself. You can enable reverse sync for your imported Databricks tags in Atlan and push all tag updates for your Databricks assets back to source. Once you have enabled reverse sync, any Databricks assets with tags updated in Atlan will also be updated in Databricks. To enable reverse sync for imported Databricks tags: From the left  of any screen, click Governance. Under the Governance heading of the _Governance cente_r, click Tags. (Optional) Under Tags, click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. In the left  under Tags, select a synced Databricks tag - synced tags will display the Databricks icon next to the tag name. On your selected tag page, to the right of Overview, click Synced tags. Under Synced tags, in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Databricks. In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel. Now when you attach Databricks tags to your Databricks assets in Atlan, these tag updates will also be pushed to Databricks! üéâ Did you know?Enabling reverse sync will not trigger any updates in Databricks until synced tags are attached to Databricks assets in Atlan.Tags:connectorsdatacrawlPreviousHow to extract on-premises Databricks lineageNextWhat does Atlan crawl from Databricks?PrerequisitesImport Databricks tags to AtlanView Databricks tags in AtlanPush tag updates to Databricks
--------------------------------------------------------------------------------

